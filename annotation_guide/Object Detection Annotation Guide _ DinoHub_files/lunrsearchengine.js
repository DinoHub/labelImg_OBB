
var documents = [{
    "id": 0,
    "url": "https://dinohub.github.io/archived_posts/2022-07-16-TIL/",
    "title": "AI Camp 2019 - Pose Classification",
    "body": "ProblemThe inaugural AI Camp presented the task of pose classification to students from Junior Colleges, Polytechnics, and Universities. An in-house image dataset of 15 poses was collected by the competition architects for the competition. The poses ranged from full-body poses such as the yoga Warrior Pose and Kung Fu Crane, to hand poses such as Spiderman and Handshake. The goal of the competition was to build a deep learning method which could learn how to classify the poses in a generalised way to achieve high classification accuracy on unseen images of the poses.  A week prior to the start of the camp, participants were given basic training on deep learning methods for image classification, and a training and validation set of 11 poses were released. The participants had one week to come up with their own methods to perform the pose classification. On the first day of the camp, the participants were given a test set they had never encountered before to check the performance of their methods. Thereafter, a curveball was released to them with an additional four poses to classify. The participants were given another test set to evaluate the performance of their methods on all 15 classes. Finally, the camp was rounded off with an evaluation against a final test set. The finalists were chosen based on the teams’ performances on the final test set. The finishing order for the finalists was eventually decided after the finalists presented their methods in a pitch to the judges. As the competition architects, Principal Engineer Eugene Ang and Engineer Ling Evan from Digital Hub prepared their own solution to the task. SolutionA combination of multiple methods were applied to deep learning to obtain optimal results. Data Augmentation: In order to increase the size and variety of images in the dataset, data augmentation was applied. During the training of the deep learning model, random augmentations were applied to the training images. The brightness of the images were randomly increased and decreased, and the images were zoomed in and out, sheared, flipped, and cropped randomly. This meant that the model would train on slightly different images each epoch and would learn to generalise and only pick up on key features that defined a class. Data augmentation essentially increases the size of the training set and provides the deep learning model with more “experience” with images that may more closely resemble images from the test set. This can provide several percentage points of accuracy increase.  Test-Time Augmentation: Test-time augmentation is a variant on data augmentation and ensembling. Instead of augmenting the images at training time, images are augmented at test time while being fed into the model. The same minor augmentations are applied of brightness, zooming, shearing, and cropping in set controlled amounts to each image. This process is repeated multiple times with different augmentations applied. The predictions for the image are aggregated and the class with the highest confidence is chosen as the predicted class. This process reaps the benefits of ensembling and instead of varying the model used to predict the test images, the test images are varied. This method can provide a slight increase in training accuracy. Pretrained ModelDeep Convolutional Neural Networks are used to perform feature extraction and image classification. These neural networks can be quite large with upwards of one hundred layers. Networks with good architecture for sufficient performance are deep and are extremely tedious to build and train from scratch. As such, pretrained models are used. Models such as ResNet50 and Xception were used as they are relatively compact and provide excellent image classification performance. A pretrained model trained on imagenet is taken with its weights and used as a base model for the custom image classification task. The final few layers of the pretrained models are removed and a Global Average Pooling layer and a final Dense layer with the corresponding number of nodes as classes in the custom dataset are added. For this task, an output dense layer of size 11 would be built first, then discarded and a new layer of size 15 added after the curveball challenge. In this configuration, the model is fine-tuned for around 20 epochs to achieve the best accuracy on the new custom dataset. Early layers may be frozen to prevent their weights from being updated during the backpropagation phase of training. This allows the training time to be drastically shortened due to the reduced number of parameters, while at no cost to performance as theses early layers extract general features and are unlikely need much changing. The use of pretrained models can reduce the amount of time and resources taken to classify a new custom dataset while providing good performance as the model is already trained to extract features. Human DetectionIn order to make sure the deep learning models are learning features of the human doing the pose and not the background, human detection is deployed to remove the background by creating a tight crop around the human. You-only-look-once (YOLOv3) was used for this task. YOLOv3 is an object detector and classifier that identifies and object and draws a bounding box around it. It is also trained to classify the identified object. YOLOv3 was modified to show only the largest person object identified in each image, which is assumed to be the person performing the pose. The bounding boxes are cropped out to create a training dataset with a tight crop around the human performing the pose. This method prevented the deep learning models from learning irrelevant features in the background and can provide a slight increase in training accuracy.  EnsemblingEnsembling is the aggregation of multiple deep learning models. Several models are trained with slight differences. Different model architectures such as ResNet50 and Xception, with different cross-validation splits, and different training hyperparameters such as optimizer and learning rate are trained on the training set. During testing, each model performs the test and produces a softmax prediction vector for each test image. In a simple average, the predictions for each image from each model are summed and the class with the highest total confidence is selected as the predicted class. This method leverages on the idea that different models classify different images correctly and the collective wisdom of the ensemble allows it to perform prediction with a higher accuracy than any individual model. To further improve the ensemble, a selective ensemble of the best-performing models can be taken. A grid-search method was created by a Digital Hub intern Ivan to iterate through all combinations of available models to find the best possible combination to ensemble. Model DistillationThe ensemble method provides an accuracy boost to the task, but comes at a cost of being very bulky and slow to run. Several models have to perform the prediction on the test set, and the ensemble would consume a significant amount of memory and time to run. As such, it is beneficial to have a lighter model with the performance of the ensemble. Model distillation is used to creating training data for such a model. After an ensemble is created, each training image is tested with the ensemble to obtain the aggregate prediction softmax vector for that image. The soft vector is then used as the ground truth instead of a hard one-hot encoded vector for the distilled model. The model trained with the soft ground truth labels can reach closer to the performance of the ensemble than the those trained on the conventional hard one-hot encoded labels. This is because there are inherent similarities in certain poses, such as HandGun and Spiderman, that should reflect in the prediction confidence that a hard label does not encapsulate. This method can produce ensemble-like performance in a smaller, lighter, single-model package. ResultsOverall, the combination of methods with an ensemble of ResNet50 and Xception models trained on the cropped images achieved a test accuracy on the final test set of 0. 861719. Not a bad result! "
    }, {
    "id": 1,
    "url": "https://dinohub.github.io/404.html",
    "title": "404",
    "body": "404 Page does not exist!Please use the search bar at the top or visit our homepage! "
    }, {
    "id": 2,
    "url": "https://dinohub.github.io/about",
    "title": "DinoHub Blog For the Dinos",
    "body": "Well, someone come up with a nice about? "
    }, {
    "id": 3,
    "url": "https://dinohub.github.io/categories",
    "title": "Categories",
    "body": ""
    }, {
    "id": 4,
    "url": "https://dinohub.github.io/",
    "title": "Home",
    "body": "      Featured:                                                                                                                                                                                                                 Starter Code for Quick Web Prototype                                                 1 2 3 4 5                                              :               Need to build a web prototype but no idea where to start? No worries! Ernest is here to the rescue. This post will provide you. . . :                                                                                                                                                                       ernestlwt                                10 Sep 2020                                                                                                                                                                                                                                                                                                                                          What does the header of each post do?                                                 1 2 3 4 5                                              :               1 2 3 4 5 6 7 8 9 10 --- layout: post title:  How to set ratings  //Sets the title of the post author:. . . :                                                                                                                                                                       Kah Siong, a. k. a Jax                                14 Jul 2020                                                                                                                                                                                                                                                                                                                  How to create a new post?                                                 1 2 3 4 5                                              :               1 2 3 4 5 6 7 8 9 10 11 12 --- layout: post title:  How to create a new post?  author: jax categories:. . . :                                                                                                                                                                       Kah Siong, a. k. a Jax                                14 Jul 2020                                                                                                                                                                                                                                                                                                                  How to add profile of a new blogger here??                                                 1 2 3 4 5                                              :               Edit _config. yml and add your profile under the authors node like follows. :                                                                                                                                                                       Kah Siong, a. k. a Jax                                14 Jul 2020                                                                                                                                                                            All Stories:                                                                                                     Pathological voice classification with deep-learned embeddings from voice recordings                         1 2 3 4 5                      :       Pathological voice classification with deep-learned embeddings from voice recordings*Work done by Gabriel Ng as an intern with Kah Siong at DH:                                                                               DigitalHubInterns                28 Sep 2020                                                                                                                                     Flight Trajectory Anomaly Detection with LSTM Classifier &amp; QuickBundles                         1 2 3 4 5                      :       With so much data out there, we are finding an increasing number of use cases for such data. GPS and other types of location data are very useful especially under. . . :                                                                               DigitalHubInterns                28 Aug 2020                                                                                                                                     Object Detection Annotation Guide              :       This blog post is meant for annotators who are embarking on their journey to help label data for object detection. We hope to briefly introduce you to the computer vision. . . :                                                                               levan                21 Jul 2020                                                                                                                                     NewtonFool              :             Overview   Limitations   Performance   Feedback   :                                                                               cvappstore                16 Jul 2020                                                                                                                                     Fantastic Hypebeasts and Where to Find Them              :       Hypebeast 21’st century fashion can be confusing. Teenagers queueing for hours outside branded boutiques to buy thousand-dollar kicks (shoes), only to pose for some photos in them before selling them. . . :                                                                               kayne_ivan_west                16 Jul 2020                                                                                                                                     Training a Open Source ML/DL model on AI Platform (Kubernetes) - Part I              :       Training a Open Source ML/DL model on AI Platform (Kubernetes) - Part I. :                                                                               Kah Siong, a. k. a Jax                13 Jul 2020                                               &laquo; Prev       1        2        3      Next &raquo; "
    }, {
    "id": 5,
    "url": "https://dinohub.github.io/page2/",
    "title": "Home",
    "body": "{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}       All Stories:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 6,
    "url": "https://dinohub.github.io/page3/",
    "title": "Home",
    "body": "{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}       All Stories:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 7,
    "url": "https://dinohub.github.io/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 8,
    "url": "https://dinohub.github.io/Pathological-voice-classification-with-deep-learned-embeddings-from-voice-recordings/",
    "title": "Pathological voice classification with deep-learned embeddings from voice recordings",
    "body": "2020/09/28 - Pathological voice classification with deep-learned embeddings from voice recordings*Work done by Gabriel Ng as an intern with Kah Siong at DH Since the covid-19 pandemic began, the significance of pathological voice classification has skyrocketed, with many countries and hospital scrambling to find an efficient means of testing their people. Given the need for specialised equipment and skilled staff to conduct a covid-19 swab test properly, it is no surprise that organisations are searching for another means of testing. A potentially more efficient method could be leverage machine learning to classify pathological voices from voice recordings. With such testing, it will be possible to diagnose people in real-time, saving on manpower. Even if such testing alone cannot serve as a diagnosis, it can be utilised to reduce false positive and negative rates of the true test. This project explores the different machine learning techniques for achieving the goal of pathological voice classification and proposes the use of a deep-learning model called RawNet. This post will cover the following content:  Literature review of current research Pathological voice datasets available Baseline model for benchmarking RawNet deep learning model Future work AppendixThe links to current research, datasets and the data used in this project will be included in the appendix Literature Review: Although machine learning in audio is less established than fields like computer vision in the area of pathology, there is still plenty of pathological voice classification research to learn from. This research can be broken up into two main sections; classification with handcrafted features and classification with deep-learned features. Handcrafted Features: Handcrafted features can be further divided into two categories; acoustic features and spectral features. Acoustic features represent characteristics of the human hearing system and the speech production system while spectral features represent the characteristics of the voice recording in the audio spectrum. To illustrate this difference, an acoustic and spectral feature is compared. An example of an acoustic feature are the Mel-frequency cepstral coefficients (MFCCs) are a representation of the audio’s short-term power spectrum on the nonlinear mel audio scale. Since the mel audio scale approximates the human auditory response to sound, the MFCCs are a representation of how humans will perceive the audio. On the other hand, spectral energy is a representation of how the energy of the audio is distributed within the different frequencies, a true representation of the audio’s characteristics. Other examples of handcrafted features are:  Harmonics to Noise Ratio Jitter and Shimmer Spectral Entropy Energy Fundamental FrequencySome of these features are utilised in the creation of the baseline model and will be explained in more detail later. Deep-Learned Features: Deep-learned features are features extracted from the audio recordings by a deep neural network (DNN). Recent research has experimented with this approach since deep-learned features might be a better representation of an individual’s voice as compared to handcrafted features. There is no guarantee that a handcrafted feature is a good discriminator for pathological voices while deep-learned features are specifically extracted by the DNN to distinguish different voice disorders. Other than RawNet, there are other open-source deep learning models for extracting deep-learned features. These are X-vector DNNs and Pretrained Audio Neural Networks (PANNs). Datasets: There are a number of open-source pathological voice datasets available. These datasets are the Saarbruecken Voice Database (SVD), VOICED database and SLI-Database. Other databases such as the Massachusetts Eye and Ear Infirmary Database and the TalkBank pathology databases are unfortunately not open-source. In this project, the data used is obtained from the SVD because the data comes in . wav files, making it easier to start experimenting with the data. In all, all samples from 6 different disorder labels were downloaded and used for this project. Within these 6 disorders, the data was split into different demographics, separated by gender and age group. There are two genders, male and female. There are three age groups, 1 to 40, 41 to 65 and 66 and up. An example of a demographic is dysphonia_female_1_40. This data can be downloaded here: https://drive. google. com/file/d/1P6Pip3ZWzKfG74p0EuWwSguIsOyAUFD_/view?usp=sharing  Dysphonia Functional Dysphonia Hyperfunctional Dysphonia Laryngitis Recurrence Palsy HealthyThere are 17521 files in the entire dataset. However, a majority of these files come from healthy voices (~8200 files) with some disorder demographics only having less than 100 files. Therefore, the dataset was processed in two different ways. In both methods, a 60-20-20 train-val-eval split was used. First, all files from the same voice disorder (e. g dysphonia) were group together irregardless of age or gender. This created a dataset of ~5400 files with ~900 files for each voice disorder. For a 60-20-20 split, this resulted in a training set of ~3240 files and 540 files per voice disorder. The validation and evaluation set each had ~1080 files with 180 files per voice disorder. Second, 5 high data demographics with at least 1000 utterances were selected (e. g healthy_male_41_65) to view the effects that additional data could have when modelling. With a 60-20-20 split, this results in a training set of 3000 files and 600 files per demographic. The validation and evaluation set each had 1000 files and 200 files per voice disorder. The quantity of data for each of these 5 demographics is not much higher than the quantity of data for each disorder in the first distribution. However, relatively, there is a much greater quantity of data on each demographic since each disorder in the first distribution contained 6 different demographics. Baseline Model: Before experimenting with RawNet, a baseline model was created to determine how difficult it would be to classify pathological voices with handcrafted features. Before extracting these handcrafted features from the voice recordings, Auditok was used to extract single utterances from the overall recordings of the SVD dataset. An example of a single utterance is ‘ooo’ or ‘aaa’ and they were extracted to their own . wav files as each original voice recording had multiple utterances with silences in between, which would affect the values of the features extracted. openSMILE was then used to extract from each utterance, the handcrafted features below.  MFCCs (Explained earlier) Spectral Energy (Explained earlier) Jitter and Shimmer. These two features represent the frequency instability and amplitude instability in the sound respectively.  Harmonics to Noise ratio is the ratio of energy in the sound’s periodic signal to the energy in the sound’s noise. Baseline Results: The handcrafted features were then used to predict which disorder label the utterance belonged to.  Multi-class classification (6 disorder classes): ~0. 22 F1-score Multi-class classification (5 high-data demographics): ~0. 31 F1-score 1 vs 1 classification of disorder classes: ~0. 55 F1-scoreThe 5 high data demographics in this result are the 5 demographics that had the highest number of single utterances after data processing of around 1000 utterances. For example, healthy_male_41_65 is a demographic with this number of utterances, allowing for more training data. The baseline results indicate that the model is doing little better than randomly guessing which disorder label should be given to an unknown utterance. This is consistent across multiple models like Random Forests, Support Vector Machines and XGBoost. However, the hyperparameters of the models were not tuned so doing this may improve the baseline results. Deep-learned Features with RawNet: To improve on the baseline results, a deep learning model called RawNet is used. The original purpose of RawNet was for speaker verification with raw audio waveforms as input. To achieve their goal of speaker verification, the researchers used a cosine similarity classifier as the last layer of RawNet. This classifier indicated how similar two embeddings from different recordings were similar to one another. The picture below illustrates the input and output of the model as well as its architecture.  This project has adapted it for pathological voice classification by treating each demographic (e. g male, dysphonia, 41-65) as speakers. The rationale for this was because based on current research, the different demographics should have distinct characteristics from one another that would enable RawNet to distinguish between the different demographics. For example, male individuals would have a lower pitch than female individuals while individuals with a voice disorder would have a higher jitter and shimmer than healthy individuals. Verifying the performance of RawNet: Before adapting RawNet for classifying pathological voices, the original research was verified by replicating the research. This was done by training the RawNet model on voxCeleb, a dataset of speech recordings from celebrities. From the visualisation of the embeddings produced from the evaluation recordings (unseen by the model during training) by RawNet after training, it can be seen that all the speakers are in their own distinct clusters. Thus, it can be concluded that the original research was indeed successful in its purpose.  Final Results: The classification with the RawNet model was conducted using the same scenarios as the baseline which utilised the SVD dataset mentioned under Datasets.  Multi-class classification (6 disorder classes): ~0. 26 F1-score Multi-class classification (5 high-data demographics): ~0. 46 F1-score 1 vs 1 classification of disorder classes: ~0. 55 - 0. 80 F1-scoreThe second multi-class situation indicates that although the model’s performance is unsuitable for deployment at the moment, given enough data, the model could potentially discover good discriminators of the different disorders. Additionally, within the 1 vs 1 classification scenario, there was a discrepancy between the F1-scores of healthy vs disorder and disorder vs disorder classifications. From this, it can be concluded that the characteristics of a healthy voice is likely to be more distinct from voices with a disorder. However, discriminating between the different disorders is likely to be more challenging since different disorders may affect an individual’s voice in similar ways. Visualisation of Embeddings: To understand the final results in more detail, several visualisations of the embeddings obtained from the evaluation data have been created. Disorder Visualisations: The two TSNE plots below show the female population (left) and male population (right), with the different colors of the dots indicating different disorders. Clearly, there are no distinct clusters formed in either population, indicating that there is no distinction between the embeddings from the disorders. This supports the earlier hypothesis that it is challenging to find a good discriminator between the different disorders.  High-data Demographics Visualisations: Additionally, the TSNE plot below visualising the 5 high-data demographics show a small degree of clustering starting to occur, with a majority of the blue dots in the top left, majority of the red in the top right, and a number of light blue at the bottom. This also shows that with enough data, RawNet might be able to distinguish between the different demographics that it is trained on.  Gender Visualisations: Furthermore, the red dots representing the healthy male demographic are clearly separated from most of the other dots which are from female voice recordings, indicating that the model could perhaps distinguish gender. Further analysis with several visualisations of male vs female for each disorder confirmed this hypothesis. Examples of these visualisations are below where male vs female for healthy individuals is on the left and male vs female for individuals with recurrence palsy is on the right.  Healthy vs Disorder, Disorder vs Disorder Visualisations: Finally, the visualisations of embeddings for healthy vs disorder and disorder vs disorder scenarios also support the earlier hypotheses on the performance of the RawNet model. In the TSNE visualisations below, each figure shows the embeddings from a healthy vs disorder scenario, in particular, healthy vs recurrence palsy for males and healthy vs functional dysphonia for females. The different disorders are marked by different colors.  The TSNE embeddings below show the embeddings from disorder vs disorder scenarios, recurrence palsy vs laryngitis for males and functional dysphonia against hyperfunctional dysphonia for females.  From these 4 visualisations, it can be seen that although there are no distinct clusters in the healthy vs disorder scenarios, the dots are still more separated than in the disorder vs disorder scenario. In the latter, the dots are evenly distributed, indicating that RawNet has found no effective discriminator to distinguish the disorders. Summary: In summary, although the SVD RawNet model in its current state is unable to discern between the multiple disorders, it is however, able to distinguish certain voice characteristics. For example, it can easily discern male voices from female voices and it also performs relatively well discerning healthy voices from unhealthy voices. To continue improving its performance in distinguishing disorders, more data is likely required. The RawNet model also performed better than the baseline model, indicating the there is potential for deep-learned features to be more effective discriminators of what represents a particular voice disorder than handcrafted features. The table below indicates the difference in performance.  Future Work: Much work remains even though this project has ended. Listed below are some of the additional steps that could be taken to take this project forward.    As mentioned earlier, the collection of additional pathological voice data to augment the current SVD dataset will likely boost the performance of RawNet significantly.     Experimentation of other classifiers like Hidden Markov Models and Gaussian Mixture Models should also be carried out to continue improving the performance of pathological voice detection with handcrafted features.     RawNet should also be compared to other models that produce deep-learned audio features like X-vectors and PANNs.  Appendix: Links to research:  Intra- and Inter-database study for Arabic, English and German databases: Do Conventional Speech Features Detect Voice Pathology?https://www. sciencedirect. com/science/article/pii/S0892199716301837  Front-End Factor Analysis For Speaker Verificationhttps://ieeexplore. ieee. org/document/5545402  Identifying distinctive acoustic and spectral features in Parkinson’s diseasehttps://www. researchgate. net/publication/335829678_Identifying_Distinctive_Acoustic_and_Spectral_Features_in_Parkinson’s_Disease  Vocal Folds Disorder Detection using Pattern Recognition Methodshttps://ieeexplore. ieee. org/document/4353023  Deep Neural Network Embeddings for Text-Independent Speaker Verificationhttps://danielpovey. com/files/2017_interspeech_embeddings. pdf  X-Vectors: Robust DNN Embeddings for Speaker Recognitionhttps://www. danielpovey. com/files/2018_icassp_xvectors. pdf  Pathological Speech Detection using X-Vector Embeddingshttps://www. researchgate. net/publication/339641784_Pathological_speech_detection_using_x-vector_embeddings  RawNet: Advanced end-to-end deep neural network using raw waveforms for text-independent speaker verificationhttps://www. researchgate. net/publication/335829649_RawNet_Advanced_End-to-End_Deep_Neural_Network_Using_Raw_Waveforms_for_Text-Independent_Speaker_Verification  openSMILE – The Munich Versatile and Fast Open-Source Audio Feature Extractorhttps://www. researchgate. net/publication/224929655openSMILE–_The_Munich_Versatile_and_Fast_Open-Source_Audio_Feature_Extractor Links to Datasets:    Saarbrucken Voice Database: http://stimmdb. coli. uni-saarland. de/index. php4#target     VOICED Database: https://physionet. org/content/voiced/1. 0. 0/     SLI-Database: https://lindat. mff. cuni. cz/repository/xmlui/handle/11372/LRT-1597  "
    }, {
    "id": 9,
    "url": "https://dinohub.github.io/Starter-Code-For-Quick-Web-Prototype/",
    "title": "Starter Code for Quick Web Prototype",
    "body": "2020/09/10 - Need to build a web prototype but no idea where to start? No worries! Ernest is here to the rescue. This post will provide you with some starter code using SOTA popular frameworks which you should have no problem finding stackoverflow answers to your questions. DisclaimerI am no expert, but this starter code should be easy to use for both development and deployment. I intend to improve the github repo as I gain more knowledge on this. So click here for more, click here to subscribe and dont forget to click the notification bell. About the RepoYou can find most of the information on the readme of the repository already. So really, there is not much for me to go through that is not repeating the readme on the repo. There are instructions on both running the servers during development and deployment. The folder structure are just something I would use. Go ahead and organize as you like. If you find a better way to structure organize the folders, do let me know! It would be helpful to me and to others. Tips on ReactjsReactjs has gone through many changes. When I first started learning, I followed the popular tutorials which does not equip you with the new features of react. So below are some of the tip I would have wish someone told me before I started learning React. Functional Components vs Class Components: 1234// class componentsclass Person extends Component {  . . . }1234// functional componentsfunction Person(){  . . . }Whats the difference? Well not so recently they introduce react hooks, which is really powerful and it is only available for functional components. So if you are just starting out, I would highly recommend you do things the functional components way. So far, I have not met anything that I could not achieve with functional components and would require class components. React Routing: React is a Single Page Application(SPA). This means that there is just one html file, irregardless of how many different url you have, and javascript does the rest of the work to change the look of the page. If you have created multiple url, make sure you use some form of routing features instead of &lt;a href&gt; as you would lose the state of the web application(you will understand this as you continue learning react). There are many different routing packages but the one used in the starter code is react-router and you can find the tutorial here. No jquery: If you have done any web application before, you would probably have used jquery before. JQuery allows you to change the webapp without reloading the page, however, react does this too! Using them both at the same time could result in unpredictable results. So no jquery for you. React should be able to do whatever you need Tips on FlaskFlask is pretty straight forward. It is design to be light-weight and you add extensions as you develop instead of them providing everything to you in the case of django. I prefer flask but your usage might vary. Structure? No Structure!: As mentioned, flask is meant to be lightweight and no folder structure is provided by default. For micro-services doing just a single task, you might not even have more than 100 lines of code, creating a folder structure might just be a waste of time. However when you start adding features to your prototype, you might start to scratch your head on how to organize your code. Thus, I have included a recommended folder structure. Use Gunicorn and Nginx: The server that comes with flask when you flask run is only for development. You might face issues using it during deployment when the number of requests/second gets higher etc. Gunicorn is a server that can serve your flask python code and it is production grade so use that instead. However, it might not be the most efficient for static files. No worries, you always have Nginx to do that for you. In fact, the react codes are compiled into static files and served using a docker container with Nginx, and the docker container for the backend server is already using Gunicorn! Database migration: Database migration is to database schema as what git is to codes. Database migration lets you keep track of the changes you made to the schema of a database. The changes you make for the database on your development machine can be easily replicated on your colleague or deployment machine. The starter code has this feature added but I have abstracted this. I figured that for prototyping purposes, you might not even have a database, or could just delete the whole database since there are not much data on it. To be in control on this, refer to the official docs of Flask-migrate to learn how. Basically there are upgrade commands like flask db upgrade, roll back commands like flask db downgrade and many more. ConclusionOnce again, I am no expert, so if you ever get to use this starter code, please let me know what you think! I intend to continue improving this article and the repo. Github link: https://github. com/ernestlwt/prototype_starter_pack PS: please read the readme"
    }, {
    "id": 10,
    "url": "https://dinohub.github.io/anomaly-detection/",
    "title": "Flight Trajectory Anomaly Detection with LSTM Classifier & QuickBundles",
    "body": "2020/08/28 - With so much data out there, we are finding an increasing number of use cases for such data. GPS and other types of location data are very useful especially under circumstances where there is a lot of repeated activity and there is a need to identify unusual activity. It could be applied to, among other things, robot data in warehouses, ground and air traffic control and prediction, and even medical imaging like brain scans. This project explores the use of labelled data to construct a model that can be used to flag anomalous, future unlabelled data, in the context of flight trajectories. Trajectory Anomaly Detection, intuitively speaking, should be an easy task, at least to humans. If we have airplanes sending out location pings periodically over the course of their flights, all it takes is for us to plot out those ping locations on a map, identify what common routes there are, and what trajectories deviate from these common routes. In machine learning parlance, these are the tasks of clustering and outlier detection, and requires nothing more than a pre-existing dataset from which to detect anomalies (i. e. the definition of “anomaly” is contextual and changes from dataset to dataset). This post outlines the following content areas:  The Dream Algorithm LSTM for Classification and Anomaly Detection Clustering - 1 Trajectory: 1 Cluster Clustering - 1 Trajectory: Multiple Subclusters (success) Summary Using the CodeThe Dream Algorithm: The fantasy is to have an algorithm that is so general that it can be applied to any trajectory type (flights? roadtrips?) regardless of context (no labels required) so that minimal manual feature-encoding / data preprocessing is required; all we have to do is to throw at it a large CSV file of clearly labelled trajectories, and it’d be able to identify trajectories that seem anomalous. But of course one can only dream :(. In reality, data is messy, and the following problems stand in the way of general clustering algorithms:  trajectories are incomplete and have variable length trajectories have different start and end-points location pings are irregularly distributed over time significant proportion of pings could have been sent while the plane was still on the ground trajectories may be noisy and have outlier pings (perturbations)As clustering algorithms, at their essence, aim to associate datapoints (in our case, trajectories) that have a short distance to each other into clusters, having all these inconsistencies is a large hindrance to being able to do consistent, fair comparisons between trajectories to obtain distance measures. For example, flight A and flight B may share the same flight route each send out 500 pings over the flight duration, but if 100 of flight A’s pings were sent before takeoff, and flight B’s pings were all sent after takeoff, the distance measure between flight A and flight B would be artificially inflated because of the extra apparent distance between the two flights during the first 100 pings. Since clustering is not a new task by any means, much work has been done on it, and indeed, there is a promising algorithm that tackles this specific problem of temporally warped data, wherein the shapes of 2 series of data are basically the same, but compressed and stretched differently over time. This algorithm is called Dynamic Time Warping (DTW), but whereas a simple Euclidean distance measure has a time-complexity of O(N), DTW has a time-complexity of O(N2). In a dataset of about only four thousand flights, each interpolated to only about 50 pings regularly spaced over their respective time-of-flight, this easily exploded the clustering time from about 10 minutes, to about 5 hours. In addition, due to all these artificially inflated distances, the distance threshold for any trajectory to be considered part of a cluster would have to be high. From trials with QuickBundles (explained later), it was found that even with a distance threshold of between 200km and 300km, about 80% of trajectories were in clusters of 5 of fewer, meaning that not many meaningful clusters were found, and that most trajectories were “far” away from each other. With such a high threshold, any comparison between fully formed trajectories from 2 different flight routes stood a good chance of being associated together as well. There are some approaches to try to standardize the data, and they include:  providing airport location data to discern when a plan has truly left the airport, so as to trim off pre-takeoff and post-landing segments interpolating and smoothing Taking inter-flight distance measure to be the proportion of inter-flight distance to total flight distance (of either of the trajectories being compared) …As you can tell, these are highly inflexible, hard-coded features, hyperparameters, and decisions that would have to be rethought from dataset to dataset. The dream algorithm remains unattainable for now, but if there’s an approach that fits the bill in being dynamic, general, generalizable, and runnable on unlabelled data, it would be a form of unsupervised deep-learning model that requires this flight trajectory anomaly detection question to be reformulated into a regression-type problem statement. More on that at the end of this post. Dynamic and General Enough - LSTM for Classification and Anomaly Detection?: As much as neural networks look like a cheap, easy fix for any computationally complicated task, their self-tuning properties make them the best candidate for the clustering segment of this task, just that we have to now turn this “clustering” problem into a “classification” problem, wherein class labels are needed. Fortunately, we do have class labels to use; for this project, we’ll use only flights that originate from “Singapore Changi Airport” and treat flight destinations as the class labels. In English, the neural network has to take a series of location pings, and figure out which pings are important to consider, what features to glean from it, so that it can make a prediction of what trajectory type it is. It sounds like the perfect job for Long-Short-Term Memory (LSTM) networks. The anomaly-detection mechanism would then be the confidence level of the prediction. In an ideal scenario, non-anomalous flights would be predicted with a high confidence, and anomalous or strange new trajectory types should be assigned to a class with a low confidence regardless of what class it is - ideally. Experimentation time! Data Pipeline: 1) Filter: From a commercial dataset of outgoing flights originating from Singapore Changi Airport, only flights bound for the top 23 destinations were used as “normal” or non-anomalous flights (i. e. part of training set). 2) Filter: Based on class label and airport information, an inter-airport distance is calculated, and only flights above a certain minimum percentage of completeness (with respect to the inter-airport distance), are retained. 3) Preprocessing: A slice of arbitrary duration (above minimum percentage) is taken from each flight, and interpolated to 50 timestamps and treated as a training input sample. A training label sample naturally took the form of a one-hot encoded 23-vector representing the correct destination (class) of the flight. Each timestamp was a 5-vector containing:  Latitude Longitude Altitude Heading SpeedWith a dataset of 4181 flights, several LSTM model variations built using Keras were trained on 90% (3762) of the flights and were evaluated on the remaining 10% (491) flights. It turns out that a fairly small model of 8 layers and hidden size of 32, trained on 200 epochs over slightly under an hour, performed the best. These were the statistics on the non-anomalous 10% test set:    Prediction rate: 94. 749%  Confidence mean: 96. 206%  Confidence S. D. : 10. 511% Pretty good! Anomaly Detection - Failed: I then fed in multiple unseen trajectory types (not in training set) to see what the confidence statistics were like for anomalies. The results were rather horrible; here are the statistics on a trajectory type I consider to be a good test-case for anomalies: flights to Surabaya Juanda International Airport. I consider this a good test case because even though it is near and going in a similar direction as its 2 neighboring trajectories - Jakarta and Sydney - all flight trajectories are clearly defined and distinct (either direction-wise, or length-wise) from each other. There are no other trajectories in the training set that overlapped any of these 3 flights either.    Prediction class: 97 / 104 Jakarta, 7 / 104 Sydney  Confidence mean: 98. 783%  Confidence S. D. : 5. 895%  The LSTM was obviously not senstive enough to anomalies. Such high confidence scores for anomalies were more or less consistent across all anomaly trajectory types. I removed Sydney from the training set, and the LSTM classified Surabaya flights as Jakarta flights with even higher confidence (99. 998%)! The LSTM is definitely tuning itself to identify flights as Jakarta flights, but there aren’t enough examples that are similar enough to Jakarta flights but not Jakarta flights to teach it what truly constitutes a Jakarta flight and what does not. What’s interesting is that clearly, a human can look at the 3 clusters of flights and know that they’re 3 distinct clusters, but an LSTM isn’t able to decipher that information. But if we think about it on a more fundamental level, it still boils down to concepts related to clustering. We can identify 3 clusters because the distances between flights of a single cluster are much lower than the distances between flights of different clusters, so we can clearly attribute the former to spread of a distribution and the latter to a difference of distribution altogether. An LSTM doesn’t encapsulate such comparative information well. If there’s a way to preprocess a 6th variable to capture such comparative information in the context of any single flight’s neighbors, that would certainly be a neat way to solve this problem with a single model. Unfortunately, that’s not possible, so it’s best to explore clustering as part two of this problem, and use the LSTM for its good performance in classification as a working part one. Clustering - 1 Trajectory: 1 Cluster: The idea here is to use the training data to construct trajectory-specific clusters. This means that we take the training data, group them by destination (trajectory type), and treat each group as 1 cluster. 23 trajectory types hence means 23 clusters. It is the centroid of the cluster, as well as the distribution of distances between cluster members and the parent centroid, that we’re really interested in. The anomaly detection mechanism is then to put the unseen trajectory sample through the LSTM, and to calculate the distance between this unseen sample and the centroid of the cluster that the LSTM predicted this unseen trajectory to be part of. The ideal situation would be that anomalies would show a much greater distance than non-anomalies to the point that the probability of using distance to correctly flag anomalies would be statistically significant. 1 Trajectory: 1 Cluster - Results: Results were mixed. Sometimes, the separation between non-anomaly and anomaly distance distributions was good. The following analysis of distance statistics assumes that all distributions follow a Normal distribution. To flag anomalies, we find the point of intersection of both distributions, and deem that anything that has a to-centroid distance larger than this point of intersection as an anomaly.  Jakarta (non-anomaly) flights:    Jakarta cluster member to Jakarta centroid distance mean: 47. 209km  Jakarta cluster member to Jakarta centroid distance S. D. : 28. 002km  Non-Jakarta (Surabaya) flights:    Non-cluster member to Jakarta centroid distance mean: 240. 320km  Non-cluster member to Jakarta centroid distance S. D. : 135. 508km  Statistics:    5. 9% undetected anomalies  4. 9% false positives But at other times, it was not that great…  Sydney (non-anomaly) flights:    Sydney cluster member to Sydney centroid distance mean: 588. 722km  Sydney cluster member to Sydney centroid distance S. D. : 373. 305km  Non-Sydney (Surabaya) flights:    Non-cluster member to Sydney centroid distance mean: 746. 916km  Non-cluster member to Sydney centroid distance S. D. : 91. 701km  Statistics:    11. 7% undetected anomalies  44. 8% false positives (&lt;– So high!) Other times still, just bad.  London (non-anomaly) flights:    London cluster member to London centroid distance mean: 899. 577km  London cluster member to London centroid distance S. D. : 561. 046km  Non-London (Amsterdam) flights:    Non-cluster member to London centroid distance mean: 881. 364km (EVEN LOWER THAN NON-ANOMALIES)  Non-cluster member to London centroid distance S. D. : 476. 013km  Statistics:    Irrelevant since non-anomalous distances were higher than anomalous ones  As you can see, this method was unreliable, especially for longer flights as longer flights tend to have multiple distinct routes, and the distance from a cluster member to its centroid could get quite large, to the point where a cluster centroid could be closer to one of the more deviant routes of another flight type than to one its own more deviant routes. Clustering - 1 Trajectory: Multiple Subclusters (success): I noticed that the first trial above was good because the flights were comparatively short and very well defined (only 1 distinct flight path per trajectory type). The last 2 trials on the other hand, performed badly, and this could be largely attributed to the fact that Sydney, London, and Amsterdam trajectories had more than 1 distinct flight path each. The flight path variance was large, and resulted in large intra-cluster distances. A more refined version is to run QuickBundles within each trajectory type, and to divide each trajectory into their own subclusters. This way, we can run an unseen (anomalous) flight through the LSTM, get its predicted trajectory type, and compare the flight with each of the cluster centroids of its predicted class, to pick out the cluster that is closest to it, and compare that centroid-trajectory distance (which we’ll call “shortest distance measure”) to the shortest distance measures of non-anomalous flights to their closest parent cluster centroid. If the anomaly’s shortest distance measure is above a certain threshold (to be statistically determined based on distribution of non-anomalous shortest distance measures), then it is to be flagged as an anomaly. What is QuickBundles?: Really quickly, QuickBundles is an iterative algorithm that takes a set of constant-length series data, and separates them into groups based on a specified distant metric. It first shuffles the dataset so that the order of processing is different each time, then it takes each data sample, one at a time, and compares it to pre-existing clusters. If there is a cluster whose centroid lies within a specified distance threshold of itself, it gets assigned to the cluster, and a new centroid for that cluster is computed. Otherwise, it becomes a cluster of its own. Because the number of clusters increase over time, the number of comparisons per iteration increase over time, and QuickBundles becomes, at worst, a O(N2) algorithm. But, it’s still super fast, so long as you don’t use DTW! Approach: 1) Preprocess: Preprocess as per the descriptions in the “Data Pipeline” subsection (same as for LSTM input)2) Split:   * 80% of the data gets used to build the subclusters with QuickBundles   * 20% of the data is compared against the subcluster centroids to generate a distribution of non-anomaly shortest distance measures3) Anomaly Detection:  1) Unseen data is preprocessed the same way as before  2) Flight is fed into LSTM for classification  3) If confidence &lt; arbitrary threshold confidence, flag as anomaly, else:  4) Flight is compared against subclusters of predicted trajectory type, to obtain shortest distance measure.     5) If shortest distance measure &gt; threshold distance, flag as anomaly, else non-anomaly. *To determine threshold distance, we either:   1) Calculate the point of intersection between the distribution of non-anomaly shortest distance measures and the distribution of anomaly shortest distance measures for any particular trajectory type, or…        2) Take it to be the distance at some arbitrary percentile (set to 95th percentile) of the distribution of non-anomaly shortest distance measures for any particular trajectory type. The following results are based on this method.      Results: Flag rates for well-defined anomalies    Surabaya: 96 / 103 (92. 308%)  Brunei: 47 / 47 (100. 000%)  San Francisco: 12 / 13 (92. 308%) Flag rates for anomalies with similar flight paths as non-anomalies    Amsterdam (similar to London): 11 / 35 (31. 428%)  Nanning (similar to Hanoi): 2 / 12 (16. 667%)  Flag rates for the 23 non-anomaly trajectory types (false positives):    Jakarta: 22 / 611 (3. 601%)  Bangkok: 32 / 392 (8. 163%)  Kuala Lumpur: 12 / 341 (3. 519%)  Taiwan: 52 / 178 (29. 213%)  Penang: 28 / 187 (14. 973%)  Sydney: 25 / 139 (17. 986%)  Phuket: 34 / 162 (20. 988%)  Yangon: 15 / 139 (10. 791%)  Hanoi: 16/ 90 (17. 778%)  Delhi: 7 / 65: (10. 769%)  Beijing: 31 / 100 (31. 000%)  *Hongkong: 145 / 422 (34. 361%)  *Shenzhen: 86 / 94 (91. 489%)  *Guangzhou: 105 / 177 (59. 322%)  **Shanghai: 71 / 183 (38. 398%)  **Seoul: 49 / 158 (31. 013%)  **Manila: 26 / 153 (16. 993%)  ***Haneda: 110 / 110 (100. 000%)  ***Narita: 102 / 102 (100. 000%)  **Mumbai: 34 / 81 (41. 975%)  **Dubai: 38 / 74 (51. 351%)  *: Hongkong, Shenzhen, and Guangzhou flights overlapped a lot, resulting in low LSTM confidence  **: Shanghai, Seoul, and Manila flights overlapped a lot, resulting in low LSTM confidence  ***: Haneda and Narita flights overlapped almost completely, resulting in low LSTM confidence  **: Mumbai and Dubai flights overlapped considerably, and have high deviance, resulting in high mean shortest distance measures.  Summary::  Anomaly Detection Rate for well-defined anomalies: 155 / 163 (95. 092%) Anomaly Detection Rate, inclusive of similar looking trajectories: 168 / 210 (80. 000%) False Positive Rate excluding asterisk classes: 420 / 2898 (14. 493%) False Positive Rate including asterisk classes: 1040 / 3958 (26. 276%)In reality, we would only be using this model for a subset of flights on a localized region. As the purpose of this project is to detect anomalous flights, the subset of “normal” flights we look at will be, more meaningfully, classified according to geographic characteristics, and not by destination. This means that the asterisk classes (Haneda and Narita for example) will be treated as 1 class as they are geographically almost the same. The problem of having a low LSTM confidence due to confusion between 2 extremely similar flight types will not exist. This means that we can fairly ignore the asterisk classes in the calculation of the false positive rate. Similarly, the problem of having anomalies with flight paths that are extremely similar to non-anomalous flight paths will, by definition, not exist. Hence we can also ignore the similar looking anomaly trajectories in the calculation of the anomaly detection rate. In any case, the bold percentages are rough statistics due to the small size of my dataset. If we were truly able to omit ALL non-anomalous trajectory types that showed significant similarities to other trajectory types, the false-positive rate would be even lower. However, it also means that if we had more different types of samples to test the anomaly detection mechanism, the detection rate might be slightly lower. The fact that this method reliably performs badly when giving it non-anomalous-seeming anomalies, and also performs badly when dealing with flights with large deviance / many spread-out flight paths (Dubai and Mumbai for example), shows that this method is attuned to the geographical features that we look out for in deciding if a trajectory is anomalous. This model is also relatively transparent because the only “black box” is the classification portion, which for that matter, isn’t too much of a black box either. The other parts of the model reflect physical characteristics that we humans can understand. It is also easy to add your own filters because it simply involves adding histograms to the already existing distance histograms and adding a filter (more and that in the FLAG ANALYZER portion below). *Work done by Ryan Tan as an intern at DH "
    }, {
    "id": 11,
    "url": "https://dinohub.github.io/Object-Detection-Annotation-Guide/",
    "title": "Object Detection Annotation Guide",
    "body": "2020/07/21 - This blog post is meant for annotators who are embarking on their journey to help label data for object detection. We hope to briefly introduce you to the computer vision task and your task as an annotator. 1. Introduction: Object detection is a computer vision task of localising and classifying objects of interest — There are some objects I want to find in an image frame. . where are they, and what are they? In our work, we want to train an AI model to perform this task of object detection. To do that, we need to provide the AI model with the ground-truths during training. AI models are very data-hungry, they require huge amounts of labelled data to train well. We are talking in orders of tens of thousands images. Just to clear up some jargons up front:  Annotation = Labels, to “teach” our AI models the ground-truths.  Classes = Categories of objects defined by us, for example, Cats, Dogs, Horses, etc. 2. Format: Annotations for an Object Detection task comprise of bounding boxes (bbs) and their respective class labels:  Bounding boxes: Bbs are rectangular boxes (typically not rotated, aka, aligned with the edges of the image). The bbs tightly bound out the objects of interest in the image.  Class labels: The class label is then given for each bb to say what it is — is it a cat? or is it a dog?3. Some Standards: Quality over Quantity: We prefer good annotations instead of getting annotations done fast. This is because the quality of data and its labels is critical to the training of the AI model and its eventual accuracy. Take your time and be thorough! We understand it will get repetitive and droning, so take a break when you find yourself zoning out! Take a walk, eat something. . Then return to the grind! Be Exhaustive: Often, there will be multiple objects of interest in one frame, we will need all of the objects to be annotated. A partially annotated image is worse than having no image at all, because it will throw off the AI model during its training.  Make it Tight: Bounding boxes should tightly bound the object of interest to the limits of what is considered the part of the object (for example, the cat’s tail is part of the cat, so the bounding box should include the tail). Having loose bounding box is an example of poor quality annotation.  Handling Partial Occlusions: You will definitely face cases where the object of interest is partially block by another object or obstacle. In these cases, try your best as a human to imagine/hallucinate the full extent of the object, and draw your bounding box accordingly.  4. Tracking in Video Data: Here, we will introduce a complementary task that we may require you to do on top of drawing bbs and labelling their classes, which may also aid in easing your annotation efforts. Very often, our data can be sequential frames from a video. Meaning that as you traverse through the images, you will “see” a video. We can make use of this temporal aspect to ease the load of annotation as well. Imagine a car driving across 100 video frames from left to right — instead of drawing a bb for 100 times on the 100 frames, we can make use of “tracking”. One way of doing this tracking is through interpolation. To illustrate, imagine if you just annotate the car on frame 1, frame 50 and frame 100 — these frames where you make a manual input on where the bb is at is what we call a Keyframe. So, for all the frames in between the keyframes, the tool will linearly interpolate the bb locations for you. You will still need to check through every frame afterwards. And of course, as you can imagine, this will not always work accurately for every frame and you will still have to adjust the locations slightly for some of the frames. Will update further with more details/illustrations. 5. Using the annotation tool: In DH, we leverage primarily on the open-source annotation tool CVAT. We will install the tool for you on your computer, so your main reading will be to study their user guide so you know how to use the tool effectively: CVAT User Guide. Please take some time to read this! "
    }, {
    "id": 12,
    "url": "https://dinohub.github.io/NewtonFool/",
    "title": "NewtonFool",
    "body": "2020/07/16 -       Overview   Limitations   Performance   Feedback   OverviewThe attack analyzed in this card aims to fool deep neural networks into misclassification by carefully constructing an impercivable change on the input. As an example, we start with an image of a panda, which our neural network correctly recognizes as a “panda” with 57. 7% confidence. Add a little bit of carefully constructed noise and the same neural network now thinks this is an image of a gibbon with 99. 3% confidence! This is, clearly, an optical illusion — but for the neural network. You and I can clearly tell that both the images look like pandas — in fact, we can’t even tell that some noise has been added to the original image to construct the adversarial example on the right! Attack Description: Newton fool works with a very simple rationale, finding the shortest distance possible to nudge the classification of an image from the original class, to another class. Therefore, NewtonFool is an untargeted attack, as we do not control which is class to misclassify the image on. Input: Image Output: The original class, adversarial class, and adversarial image Performance: Due to the nature of the algorithm, NewtonFool will always succeed. However, for realistic purposes, we set a X number of iteration and if the adversarial image is still not able to misclassify, we declare the attack as a failure. LimitationsAs mentioned previously, NewtonFool is an untargeted attack. In real world scenario, this might not be as useful when we require the misclassification to be of a certain class. FeedbackWe’d love your feedback on the information presented in this card and/or the framework we’re exploring here for model reporting. Please also share any unexpected results. Your feedback will be shared with model and service owners. Contact me @ elimwoon@dsta. gov. sg to provide feedback. "
    }, {
    "id": 13,
    "url": "https://dinohub.github.io/Fantastic-Hypebeasts-and-Where-to-Find-Them/",
    "title": "Fantastic Hypebeasts and Where to Find Them",
    "body": "2020/07/16 - Hypebeast21’st century fashion can be confusing. Teenagers queueing for hours outside branded boutiques to buy thousand-dollar kicks (shoes), only to pose for some photos in them before selling them on the internet for a surprisingly handsome profit. Clothes, bags, caps, shovels, and even kayaks spike in value when they are branded with stickers bearing the infamous word “Supreme”. Gram (Instagram) ‘models’ that do not let anything without a brand name touch their body so that they can impress people they do not know with wealth they do not have. These people who thirst to be part of a trend are commonly known as “hypebeast”. But what is hypebeast? Originally a derogatory term for those who favour expensive, hyped-up brands but suffer a debilitating lack of style, it has now lost its negative connotation as it promotes the lack of style as a style itself.  Popular instagrammer and self-proclaimed hypebeast Eugene Ang puts it simply: “Hypebeast is a way of life”. However, identifying hypebeast individuals is not so easy as the term is, at best, vague. As such, there was a need to create a tool to identify the style, an impartial digital judge to clear all doubts. In this article, I summarise the project to construct a convolutional neural network and train it to classify hypebeast. Collecting the dataFor this project, images were collected to use as training data for the hypebeast classifier. Web scrapers were used such as google-images-download and instagram-scraper to collect images from the internet. For google-images-download, keywords such as “hypebeast” were used to find thousands of images on google image search for download. Several famous hypebeast instagram accounts were scraped using instagram-scraper to obtain another few thousand images of fashion commonly agreed to be hypebeast. Negative examples were also obtained for the classifier to train with, and were obtained with google-images-download with keywords such as “office wear”, “normal clothes”, and “stock photo people”. In all, around 26,000 hypebeast images and 7,200 non-hypebeast images were collected. The collected images were then filtered to keep only pictures that contained humans. For this task, a pretrained ResNet50 with ImageNet weights was used. All images were fed through yolov3, an object detector and image classifier that stand for “you only look once”. Only those with 85% and above confidence of the presence of a human were kept to be used for training and testing. After filtering, around 12,700 hypebeast and 5,300 non- hypebeast images were kept. They were each split in an 8:1:1 ratio to be training, validation, and testing sets respectively.  Training the modelThe images were first passed through a data generator to augment the data slightly and produce a more rounded training set. The training images were subject to shear up to 0. 2, zoom up to 0. 2, and flipped horizontally. All images were also resized to fit the target size of 224 by 224 pixels, and pixel values were rescaled to range between 0 and 1. The data generator modified the images and fed them into the model for training in batches of size 32. Transfer learning was applied to create the hypebeast classifier. ResNet50 with ImageNet weights was used as a base model for the classifier. A global average pooling layer and two dense layers with relu activation and a softmax output layer were added behind ResNet50. This resulted in a complex and deep model with more than 24. 7 million training parameters. The model was compiled using the rmsprop optimizer on sparse categorical entropy loss and run for 20 epochs on the training set. After that, the original 50 layers of ResNet50 were frozen, and the model was recompiled with the stochastic gradient descent algorithm with a learning rate of 0. 0001 and momentum of 0. 8. The final few layers were trained a further 10 epochs using this setup to obtain a classifier that was 87. 4% accurate at predicting hypebeast images in the validation set. ResultsThe weights for the trained model were saved in a . h5 file to be used for classification. To use the model, we input a query image, and measured the confidence value for the classes “Hypebeast” and “Normal”. The class with the higher confidence value was selected to be the result of the classification, and the result and its confidence were displayed as an output. Hypebeast or not?:   *work done as an intern at DH "
    }, {
    "id": 14,
    "url": "https://dinohub.github.io/How-Does-the-header-work/",
    "title": "What does the header of each post do?",
    "body": "2020/07/14 - 12345678910---layout: posttitle:  How to set ratings  //Sets the title of the postauthor: john //Sets the author of the post, not sure if it supports multiple thoughcategories: [ Jekyll, tutorial ] //Sets the category, each category will have its own folder generated dynamicallytags: [red, yellow] //Useful for searchingimage: assets/images/11. jpg //Sets the main image for this postdescription:    //Short description of the post that's display on main blog pagerating: 4. 5 //Sets a rating, the stars will be dynamically generated---"
    }, {
    "id": 15,
    "url": "https://dinohub.github.io/Adding-a-new-post/",
    "title": "How to create a new post?",
    "body": "2020/07/14 - 123456789101112---layout: posttitle:  How to create a new post? author: jaxcategories: [jekyll]tags: [red, yellow]image: assets/images/11. jpgdescription: 'How to create a new post?'featured: truehidden: truerating: 4. 5---Create your post in markdown, add the above metadata on the top, and commit it into the _posts folder. Push it to Github and that’s it!Do note that Github has a limit of how many times it will rebuild for a Github Page, if you find your commit/push not reflected,do one of following.  Clear your browser cache Wait for the next day when rebuild resumes"
    }, {
    "id": 16,
    "url": "https://dinohub.github.io/Adding-A-New-Blog-User/",
    "title": "How to add profile of a new blogger here??",
    "body": "2020/07/14 - Edit _config. yml and add your profile under the authors node like follows. 123456789--- jax:  name: Jax  display_name: Jax  gravatar: 4650582bde3dbf1b7e74cba2cf11284f  email: jax79sg@yahoo. com. sg  web: https://jax79sg. github. io  twitter: ''  description:  Chup Cai Png Engineer ---For gravatar, go to gravatar. com to create an account and copy/paste the hash for your avatar here. "
    }, {
    "id": 17,
    "url": "https://dinohub.github.io/ml-dl-model-training-on-aiplatform-part1/",
    "title": "Training a Open Source ML/DL model on AI Platform (Kubernetes) - Part I",
    "body": "2020/07/13 - Training a Open Source ML/DL model on AI Platform (Kubernetes) - Part I.                     Difficulty          Contact   Jax@Slack   For a lot of us, we have been building and/or training our models on our own GPU enabled machines. Our single GPU enabled machines at best can cater up to 11GB of RAM, while this is sufficient for smaller models, its a challenge if we want to train larger models from scratch (E. g. BERT). Other than the scale, the speed of training on Ti1080s and RTX2080s are limited, so moving the training onto Kubernetes where V100 GPUs are available will significantly improve the above. Who should try this?: 3rd Parties: If you have a Deep Learning architecture you got from someone or pulled from open sourced research, and you need to perform some form of training on the model without intimate knowledge of the codes, this method would be most suitable for you. Development in Docker: For those who are regularly developing their codes in Docker, this would be very apt for them as well. The advantage of developing DL models in Docker is that they highly flexible when it comes to using different frameworks and versions. For example, you don’t have to crack your head on different versions of CUDA on your machine, just make sure you have a docker for every version. Most times, you don’t even have to worry about this as the frameworks such as Tensorflow come with their own docker images anyway. What will be achieved at the end of this article?: This example uses a 3rd party end to end image classification code. The code is customised to download dataset frmo S3 onto itself and also to upload model checkpoints and final results onto S3 after training. By the end of this article, you will get acquainted with very basic use of Docker and Kubernetes. You would be able to submit jobs to Kubernetes and get the results from S3 object stores. Overview: Preparation and then the model training: The above diagram is a visual representation of the steps depicted in this article. Do refer to it if you get lost along the way with the mountain of words. Prerequisites:  A client machine configured to connect to the Kubernetes cluster.  Docker installed on your own computer (Both Windows and Linux versions are fine)On your own computer:  Prepare your datasets Prepare your training codes Prepare Dockerfile file Build a docker image Export/Save the docker image as a file Transfer to Kubernetes clientOn the Kubernetes client:  Load datasets onto S3 Load the docker image file as a docker image Push the docker image to the Docker Registry Prepare kubernetes job yaml file Run the job yaml fileStep-by-Step: On your own computer: Prepare your datasets: As we are submiting the jobs to the network for training, it means that your datasets needs to be accessible by several computers on the network via a shared storage. On the AI Platform, the following would be made available, S3, NFS and Hadoop. In this article, we will demonstrate with the use of S3 storage as it can cater to both structured and unstructured data. We would be loading datasets into S3 via a network connection, so its generally more efficient if you transfer files as a zipped archive rather than thousands of individual files unless your training codes do it differently. Do zip up your datasets if you can, however, if you have extremely large datasets, or structured data in databases, you can prepare the data in their native forms. Prepare your training codes: Your training codes should consist mainly of 3 parts.  Downloading of datasets from S3 Preprocessing and training of model Uploading of models and results data to S3Finally make sure your codes can run and train for at least an epoch to verify its working. A sample of a training code is found in image_classification_single. py. The only change to this code to the original is such that the zipped datasets would be downloaded from S3 and then extracted for processing, after training, the model and results are saved in S3. Your situation could be different, please exercise your own considerations. A temporary MINIO S3 server has been setup in the AI Platform, your training codes should pull and save the data there. The following extracts the related codes from the above example. This setups the helper codes and pulls the relevant parameters about the S3. The parameters are to be sent into the environment variables. You may also hard code the variables if that suits you, but i would encourage you to use either environment variables or argparse. 12345678910111213141516171819202122232425262728293031323334353637383940### Setup of S3 parameters and helper functions#Names of the bucketstrainingbucket= os. environ['trainingbucket'] #'training'datasetsbucket= os. environ['datasetsbucket'] #'datasets'import boto3 #boto3 is S3 client from AWSfrom botocore. client import Configs3 = boto3. resource('s3',          endpoint_url= os. environ['endpoint_url'] ,          aws_access_key_id= os. environ['aws_access_key_id'] ,          aws_secret_access_key= os. environ['aws_secret_access_key'],          config=Config(signature_version= os. environ['signature_version']),          region_name= os. environ['region_name'])def s3_download_file(localpathandfilename,bucket,s3pathandfilename):  '''   localpathandfilename example: '/workspace/example. zip'   s3pathandfilename example: 'cat_dog/example. zip'  '''  print( S3 Download s3:// +bucket+ /  + s3pathandfilename +   to   + localpathandfilename)  s3. Bucket(bucket). download_file(s3pathandfilename, localpathandfilename)  def s3_upload_file(localpathandfilename,bucket,s3pathandfilename):  '''   localpathandfilename example: '/workspace/example. zip'   s3pathandfilename example: 'cat_dog/example. zip'  '''  print( S3 Uploading   + localpathandfilename +   to s3:// +bucket + s3pathandfilename)  s3. Bucket(bucket). upload_file(localpathandfilename, s3pathandfilename)  def s3_upload_folder(folder, bucket,s3path):    from glob import glob  print( Processing folder )  for file in glob(folder+ /**/* ,recursive=True):   if (os. path. isdir(file)) == False:     print( Processing   + file)    s3_upload_file(bucket='training',localfile=file,s3path='')This part of the code will save the results into S3. 123456789101112### Up until this point,allthe model files are saved on container. After this container finishes execution, the files will be gone. ### Start saving the checkpoints and model files. import jsonwith open('catdogclassification. json', 'w') as fp:  json. dump(history. history, fp)s3_upload_file(bucket=trainingbucket,localfile='catdogclassification. json',s3path='')s3_upload_folder(bucket=trainingbucket,folder='catdogclassification_model',s3path='')for epochrun in range(epochs):  s3_upload_file(bucket=trainingbucket,localfile='catdogclassification_save_at_'+str(epochrun+1)+'. h5',s3path='')Prepare Dockerfile file: Now that you have your codes ready and tested locally, its time to dockerize it. Its really easy to create a Docker image, all you need is Docker installed, gather the files you want in the docker image and to create a simple file called Dockerfile. A Dockerfile is declarative, and the commands are only processed after you run docker build. The following is the Dockerfile for this example. FROM tensorflow/tensorflow:nightly-gpuADD requirements. txt /ADD image_classification_single. py / RUN apt update &amp;&amp; \  apt install -y software-properties-common build-essential graphviz RUN pip3 install -r requirements. txtMost Dockerfiles start off with a baseline image. There are a lot of images on DockerHub and chances are that there’s one that fits your purpose. Take for example, in this case the latest Tensorflow with GPU support is desired. Instead of creating a setup with CUDA and go through all the installation headache, a pre-made docker image by Tensorflow complete with CUDA and all is used instead. To do this, a FROM command followed by the tag tensorflow/tensorflow:nightly-gpu is used. Next, copy all the stuff required into the docker image by using the ADD command, followed by 2 arguments. The first argument is the path to the file, relative to the location of the Dockerfile file. The second argument is the path inside the docker image (The folders will be created automatically if it doesn’t exists). So it will look something like ADD requirements. txt /. The codes won’t run without the dependancies. In this example, graphviz and some python packages are quired. To this end, you can use the RUN command. For this example, use RUN pip3 install -r requirements. txt. After this is acheived, you may proceed to build the image. Build a docker image: To build the image with the docker file, you need to run the following command in the same folder where Dockerfile is located. In this example, under the kubejob/single-train folder. 12345jax@getafix: docker build . t image-classification-singleRemoving intermediate container d941290dff33 ---&gt; 3793f6e38a2fSuccessfully built 3793f6e38a2fSuccessfully tagged image-classification-single:latestYou can run docker images and see the docker image listed. 123jax@getafix: docker imagesREPOSITORY                    TAG               IMAGE ID      CREATED       SIZEimage-classification-single            latest              3793f6e38a2f    2 minutes ago    3. 49GBAt this point, you can run the docker image on your own computer and run the training. This is the closest to which how it will run on kubernetes. Successfullying running this step will ensure that your image will most likely run properly on kubernetes. 1docker run -it --gpus all --env-file env. list image-classification-single python3 /image_classification_single. py--gpus all directs docker to use the GPU (provided nvidia-docker is installed) --env-file env. list loads the environment variables (S3 parameters) into the docker container. python3 /image_classification_single. py is the command to run your training script When you run the command, you would see something like following (Note that this script ensures only 1 epoch is run for testing sake) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849502020-07-11 13:45:41. 014731: I tensorflow/stream_executor/platform/default/dso_loader. cc:48] Successfully opened dynamic library libcudart. so. 10. 12. 4. 0-dev20200705S3 Download s3://datasets/kagglecatsanddogs_3367a. zip to kagglecatsanddogs_3367a. zipDeleted 1590 imagesFound 23410 files belonging to 2 classes. Using 18728 files for training. 2020-07-11 13:45:51. 001862: I tensorflow/stream_executor/platform/default/dso_loader. cc:48] Successfully opened dynamic library libcuda. so. 12020-07-11 13:45:51. 005333: E tensorflow/stream_executor/cuda/cuda_driver. cc:314] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected2020-07-11 13:45:51. 005350: I tensorflow/stream_executor/cuda/cuda_diagnostics. cc:169] retrieving CUDA diagnostic information for host: 7c20885796ae2020-07-11 13:45:51. 005355: I tensorflow/stream_executor/cuda/cuda_diagnostics. cc:176] hostname: 7c20885796ae2020-07-11 13:45:51. 005777: I tensorflow/stream_executor/cuda/cuda_diagnostics. cc:200] libcuda reported version is: 440. 100. 02020-07-11 13:45:51. 005818: I tensorflow/stream_executor/cuda/cuda_diagnostics. cc:204] kernel reported version is: 440. 100. 02020-07-11 13:45:51. 005825: I tensorflow/stream_executor/cuda/cuda_diagnostics. cc:310] kernel version seems to match DSO: 440. 100. 02020-07-11 13:45:51. 006963: I tensorflow/core/platform/cpu_feature_guard. cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 FMATo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2020-07-11 13:45:51. 036899: I tensorflow/core/platform/profile_utils/cpu_utils. cc:104] CPU Frequency: 2592000000 Hz2020-07-11 13:45:51. 038416: I tensorflow/compiler/xla/service/service. cc:168] XLA service 0x438ea80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:2020-07-11 13:45:51. 038456: I tensorflow/compiler/xla/service/service. cc:176]  StreamExecutor device (0): Host, Default VersionFound 23410 files belonging to 2 classes. Using 4682 files for validation. Corrupt JPEG data: 2226 extraneous bytes before marker 0xd9108/293 [==========&gt;. . . . . . . . . . . . . . . . . . . ] - ETA: 22s - loss: 0. 6661 - accuracy: 0. 6215Corrupt JPEG data: 228 extraneous bytes before marker 0xd9119/293 [===========&gt;. . . . . . . . . . . . . . . . . . ] - ETA: 20s - loss: 0. 6637 - accuracy: 0. 6241Warning: unknown JFIF revision number 0. 00149/293 [==============&gt;. . . . . . . . . . . . . . . ] - ETA: 17s - loss: 0. 6568 - accuracy: 0. 6288Corrupt JPEG data: 128 extraneous bytes before marker 0xd9154/293 [==============&gt;. . . . . . . . . . . . . . . ] - ETA: 16s - loss: 0. 6547 - accuracy: 0. 6313Corrupt JPEG data: 65 extraneous bytes before marker 0xd9160/293 [===============&gt;. . . . . . . . . . . . . . ] - ETA: 15s - loss: 0. 6526 - accuracy: 0. 6335Corrupt JPEG data: 396 extraneous bytes before marker 0xd9163/293 [===============&gt;. . . . . . . . . . . . . . ] - ETA: 15s - loss: 0. 6526 - accuracy: 0. 6342Corrupt JPEG data: 239 extraneous bytes before marker 0xd9293/293 [==============================] - ETA: 0s - loss: 0. 6259 - accuracy: 0. 6564Corrupt JPEG data: 252 extraneous bytes before marker 0xd9Corrupt JPEG data: 1153 extraneous bytes before marker 0xd9Corrupt JPEG data: 162 extraneous bytes before marker 0xd9Corrupt JPEG data: 214 extraneous bytes before marker 0xd9Corrupt JPEG data: 99 extraneous bytes before marker 0xd9Corrupt JPEG data: 1403 extraneous bytes before marker 0xd9293/293 [==============================] - 39s 133ms/step - loss: 0. 6259 - accuracy: 0. 6564 - val_loss: 0. 6948 - val_accuracy: 0. 5043WARNING:tensorflow:From /usr/local/lib/python3. 6/dist-packages/tensorflow/python/training/tracking/tracking. py:111: Model. state_updates (from tensorflow. python. keras. engine. training) is deprecated and will be removed in a future version. Instructions for updating:This property should not be used in TensorFlow 2. 0, as updates are applied automatically. 2020-07-11 13:46:35. 042243: W tensorflow/python/util/util. cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them. WARNING:tensorflow:From /usr/local/lib/python3. 6/dist-packages/tensorflow/python/training/tracking/tracking. py:111: Layer. updates (from tensorflow. python. keras. engine. base_layer) is deprecated and will be removed in a future version. Instructions for updating:This property should not be used in TensorFlow 2. 0, as updates are applied automatically. S3 Uploading catdogclassification. json to s3://trainingcatdogclassification. jsonProcessing folderProcessing catdogclassification_model/saved_model. pbS3 Uploading catdogclassification_model/saved_model. pb to s3://trainingcatdogclassification_model/saved_model. pbProcessing catdogclassification_model/variables/variables. data-00000-of-00001S3 Uploading catdogclassification_model/variables/variables. data-00000-of-00001 to s3://trainingcatdogclassification_model/variables/variables. data-00000-of-00001Processing catdogclassification_model/variables/variables. indexS3 Uploading catdogclassification_model/variables/variables. index to s3://trainingcatdogclassification_model/variables/variables. indexS3 Uploading catdogclassification_save_at_1. h5 to s3://trainingcatdogclassification_save_at_1. h5Export/Save the docker image as a file: The above steps ensured that you have a running training script that will work on Docker. The next step is to export the docker image so you can transfer it to the Kubernetes client. 1jax@getafix: docker save image-classification-single -o image-classification-single. tarNow transfer the docker image to Kubernetes client On the Kubernetes client: Upload datasets onto S3: On the Kubernetes client, a MINIO client (commandline) has been configured for you to manage your buckets. In this example, the following command would have been executed. 12345jax@getafix: /home/user/mc mb myminio/datasetsBucket created successfully `myminio/datasets`. jax@getafix: /home/user/mc cp kagglecatsanddogs_3367a. zip myminio/datasets/kagglecatsanddogs_3367a. zip. . . atsanddogs_3367a. zip: 786. 68 MiB / 786. 68 MiB ┃▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓┃ 55. 17 MiB/s 14sSome basic usage of the commmands are as follows. To create a new bucket ‘mynewbucket’, run the following command. 1/home/user/mc mb myminio/mynewbucketTo upload a folders or files ‘mylocalfolderorfile’, run the following command. 1/home/user/mc cp mylocalfolderorfile myminio/mynewbucket/To download a folder or file ‘mynewbucket’, run the following command. 1/home/user/mc cp myminio/mynewbucket/myremotefolderorfile mylocalfolderorfilePush the docker image to the Docker Registry: The docker image file that we copied over from our own computer needs to be loaded into the Docker Registry on the AI Platform. 12345678910111213141516171819202122232425262728293031#Loads the tar file (docker image) into the client's local docker repo. jax@getafix: docker load -i image-classification-single. tar Loaded image: image-classification-single:latest#Tag the uploaded image to bear the url to the AI Platorm's Docker Rgistry. jax@getafix: docker tag image-classification-single dockrepo. dh. gov. sg:5000/image-classification-single:latest #Send the image from local Docker to the AI Platform's docker registry. jax@getafix: docker push dockrepo. dh. gov. sg/image-classification-single:latest The push refers to repository [myregistry. com:5000/image-classification-single]94f43a58fd54: Layer already exists c394cd29e2f8: Layer already exists a7aabfd17751: Layer already exists be0113cc7bc0: Layer already exists 21f7133a99fb: Layer already exists 626976cc3d82: Layer already exists 63beefd08b72: Layer already exists e8f3214614e5: Layer already exists cc3fc5898d66: Layer already exists 7db070456ae6: Layer already exists 10a49ffdc6d4: Layer already exists 45a3946bc76a: Layer already exists 43895ac43b99: Layer already exists 808fd332a58a: Layer already exists b16af11cbf29: Layer already exists 37b9a4b22186: Layer already exists e0b3afb09dc3: Layer already exists 6c01b5a53aac: Layer already exists 2c6ac8e5063e: Layer already exists cc967c529ced: Layer already exists latest: digest: sha256:1df82e72ddb603195af7b57034d536190ccbc2c3ee59faed9a4844d3c079b8da size: 4515Prepare kubernetes job yaml file: The final step of preparation is to create a Kubernetes yaml file. 1234567891011121314151617181920212223242526272829apiVersion: v1kind: Podmetadata:  name: single-trainspec:     containers:     - name: test-image-classification-single      image: dhrepo. dh. gov. sg:5000/image-classification-single       env:      - name: trainingbucket       value: training      - name: datasetsbucket       value: datasets      - name: endpoint_url       value: http://minio. dsta. ai:9001      - name: aws_access_key_id       value: user      - name: aws_secret_access_key       value: password      - name: signature_version       value: s3v4      - name: region_name       value: us-east-1      resources:       requests:         cpu:  2          memory:  2Gi       command: [ python3 , /image_classification_single. py ]     restartPolicy: NeverThe above yml is a minimal yaml required for this example, with the important ones stated below. image - Specify the image of that the job will run. env - List the environment variables required to pass to the container. requests - Minimum resources required for this container to run command - command to run the script Run the pod yaml file: Lastly, run the pod submission. 12jax@getafix: kubectl apply -f kube-single. ymlpod/single-train createdWhen a kubernetes pod has been successfully submited, you can monitor 2 things, as indicated below. Following command will display the pod that are in Kubernetes. The pod name takes after the meta-data &gt; name in the yaml file. 1234jax@getafix: kubectl get podsNAME         READY  STATUS   RESTARTS  AGEsingle-train     0/1   Completed  0     10hThe following command will display the running stdout of the pod that you just created. The output should be identical to the docker run output above. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647jax@getafix: kubectl logs single-train2020-07-13 03:22:24. 361047: I tensorflow/stream_executor/platform/default/dso_loader. cc:48] Successfully opened dynamic library libcudart. so. 10. 12020-07-13 03:22:35. 524867: I tensorflow/stream_executor/platform/default/dso_loader. cc:48] Successfully opened dynamic library libcuda. so. 12020-07-13 03:22:35. 524895: E tensorflow/stream_executor/cuda/cuda_driver. cc:314] failed call to cuInit: UNKNOWN ERROR (-1)2020-07-13 03:22:35. 524910: I tensorflow/stream_executor/cuda/cuda_diagnostics. cc:156] kernel driver does not appear to be running on this host (single-train-94nvt): /proc/driver/nvidia/version does not exist2020-07-13 03:22:35. 525161: I tensorflow/core/platform/cpu_feature_guard. cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2020-07-13 03:22:35. 529438: I tensorflow/core/platform/profile_utils/cpu_utils. cc:104] CPU Frequency: 2592000000 Hz2020-07-13 03:22:35. 529656: I tensorflow/compiler/xla/service/service. cc:168] XLA service 0x43dac60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:2020-07-13 03:22:35. 529674: I tensorflow/compiler/xla/service/service. cc:176]  StreamExecutor device (0): Host, Default VersionCorrupt JPEG data: 2226 extraneous bytes before marker 0xd92. 4. 0-dev20200705S3 Download s3://datasets/kagglecatsanddogs_3367a. zip to kagglecatsanddogs_3367a. zipDeleted 1590 imagesFound 23410 files belonging to 2 classes. Using 18728 files for training. Found 23410 files belonging to 2 classes. Using 4682 files for validation. Corrupt JPEG data: 228 extraneous bytes before marker 0xd9 95/293 [========&gt;. . . . . . . . . . . . . . . . . . . . . ] - ETWarning: unknown JFIF revision number 0. 00Corrupt JPEG data: 128 extraneous bytes before marker 0xd9Corrupt JPEG data: 65 extraneous bytes before marker 0xd9Corrupt JPEG data: 396 extraneous bytes before marker 0xd9Corrupt JPEG data: 239 extraneous bytes before marker 0xd9287/293 [============================&gt;. ] - ETA: 1s - Corrupt JPEG data: 252 extraneous bytes before marker 0xd9Corrupt JPEG data: 1153 extraneous bytes before marker 0xd9Corrupt JPEG data: 162 extraneous bytes before marker 0xd9Corrupt JPEG data: 214 extraneous bytes before marker 0xd9Corrupt JPEG data: 99 extraneous bytes before marker 0xd9Corrupt JPEG data: 1403 extraneous bytes before marker 0xd9293/293 [==============================] - 91s 309ms/step - loss: 0. 6408 - accuracy: 0. 6454 - val_loss: 0. 6949 - val_accuracy: 0. 4957WARNING:tensorflow:From /usr/local/lib/python3. 6/dist-packages/tensorflow/python/training/tracking/tracking. py:111: Model. state_updates (from tensorflow. python. keras. engine. training) is deprecated and will be removed in a future version. Instructions for updating:This property should not be used in TensorFlow 2. 0, as updates are applied automatically. 2020-07-13 03:24:11. 022698: W tensorflow/python/util/util. cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them. WARNING:tensorflow:From /usr/local/lib/python3. 6/dist-packages/tensorflow/python/training/tracking/tracking. py:111: Layer. updates (from tensorflow. python. keras. engine. base_layer) is deprecated and will be removed in a future version. Instructions for updating:This property should not be used in TensorFlow 2. 0, as updates are applied automatically. S3 Uploading catdogclassification. json to s3://trainingcatdogclassification. jsonProcessing folderProcessing catdogclassification_model/saved_model. pbS3 Uploading catdogclassification_model/saved_model. pb to s3://trainingcatdogclassification_model/saved_model. pbProcessing catdogclassification_model/variables/variables. data-00000-of-00001S3 Uploading catdogclassification_model/variables/variables. data-00000-of-00001 to s3://trainingcatdogclassification_model/variables/variables. data-00000-of-00001Processing catdogclassification_model/variables/variables. indexS3 Uploading catdogclassification_model/variables/variables. index to s3://trainingcatdogclassification_model/variables/variables. indexS3 Uploading catdogclassification_save_at_1. h5 to s3://trainingcatdogclassification_save_at_1. h5What’s next: The above is a very simple example to demonstrate the use of Docker and Kubernetes. However, running a single training on a 32GB V100 GPU card is not efficient and running as a pod directly is not typical. The next article Part II demonstrates how this same example can be enhanced to support some form of hyperparameter tuning (E. g. Running several training jobs with different hyperparamters concurrently, as long as the total GPU ram is not exceeded). Call for contribution: The above example is one of many possible ways to utilise Kubernetes for our AI development. If you have an interesting idea, please feel free to share it on our Slack page. "
    }, {
    "id": 18,
    "url": "https://dinohub.github.io/Training-Deep-Learning-Algorithms-using-Kubernetes-on-AIPlatform/",
    "title": "Training Deep Learning Algorithms using Kubernetes on the AI Platform",
    "body": "2020/07/13 - AI Platform Advanced Developers Tutorial 1Training Deep Learning Algorithms using Kubernetes on the AI Platform By Jadle@Slack: Overview of Article: This article builds on top of the initial &lt;a href=https://github. com/jax79sg/kubejob/blob/master/single-train/README. md&gt;article&lt;/a&gt; by Jax. In his article, the basics of building a Docker image, pushing it to a Docker repository and running the training algorithm in Kubernetes is discussed. In this article, we will be training a chatbot using &lt;a href=https://www. cs. cornell. edu/~cristian/Cornell_Movie-Dialogs_Corpus. html&gt;movie dialogue&lt;/a&gt; data. The initial steps of building the Docker image and sending the job to Kubernetes are largely similar but in this case, it is more targeted towards the AI platform. Building Our Docker Image: &lt;a href=https://kubernetes. io/&gt;Kubernetes&lt;/a&gt; is an orchestration tool which automates management of containerized applications. Before we discuss how to perform training of your Deep Learning algorithm using Kubernetes, your &lt;a href=https://www. docker. com/&gt;Docker&lt;/a&gt; image needs to be uploaded into the Docker repository in the AI platform. This requires you to package all required software libraries and tools into a &lt;a href=https://docs. docker. com/engine/reference/commandline/build/&gt;Docker image&lt;/a&gt; in an Internet-enabled machine with Docker installed built. 1docker build -t my_tensorflow:2. 2. 0-gpu-dialogue . In this example, our dockerfile is relatively straightforward. Since the docker image that we pull is running as the root user, we do not need to specify sudo in our dockerfile. 12345FROM tensorflow/tensorflow:2. 2. 0-gpuRUN apt-get update &amp;&amp; apt-get -y upgradeRUN pip install nltk boto3 numpy pandas pickleCOPY deep_learning_train. py /home/deep_learning_train. pyWORKDIR /home/Check that your Docker image has been successfully built using docker image list. Assuming that the build was successful, you should see your docker image in the list of Docker images available locally. After your image is built, save via: 1docker save --output my_docker_image. tar my_tensorflow:2. 2. 0-gpu-dialogueThe extracted docker image my_docker_image. tar will be saved in the path where you ran the command. Now, you can transfer your Docker image to the client machine before uploading it to Docker repository in the AI platform. In the client machine, run the following commands: 123docker load my_docker_image. tar my_tensorflow:2. 2. 0-gpu-dialoguedocker tag my_tensorflow:2. 2. 0-gpu-dialogue dockrepo. dh. gov. sg/my_tensorflow:2. 2. 0-gpu-dialoguedocker push dockrepo. dh. gov. sg/my_tensorflow:2. 2. 0-gpu-dialogueOnce the Docker image is pushed into the Docker repository of the AI platform, we can proceed to use Kubernetes to train our model using the AI platform’s computing resources. Deploying Our Image in Kubernetes: Before calling Kubernetes, we first need to define a . yaml file. This file specifies key information for Kubernetes, including which image to pull, as well as the optimal amount of resources which you require while training your model. Do note that the pod will not run if Kubernetes is unable to assign the required resources specified, so please refrain from requesting large amount of resources. Let us now look at how to define a &lt;a href=https://kubernetes. io/docs/concepts/overview/working-with-objects/kubernetes-objects/&gt;. yaml&lt;/a&gt; file. We remark that the number of spacing between the different levels is not important, as long as it is consistent. 12345678910111213141516171819202122apiVersion: v1kind: Podmetadata: name: dialogue-train labels:  purpose: dialogue-trainspec: containers: - name: dialogue-train-container  image: dockrepo. dh. gov. sg/my_tensorflow:2. 2. 0-gpu-dialogue  resources:   limits:    cpu:  8     memory:  64Gi     nvidia. com/gpu:  1    requests:    cpu:  4     memory:  48Gi     nvidia. com/gpu:  1   command: [ python ,  /home/dialogue_trial_transformer_tf_ver2. py ]  args: [ -b ,  128 ,  -d ,  100 ,  -n ,  500000 ] restartPolicy: NeverWe bring to your attention the following parameters. Under spec, the image specifies the Docker image to pull from the repository, while the resources indicate how much compute resources your code is requesting. One interesting thing to note is that Kubernetes is able to &lt;a href=https://kubernetes. io/docs/tasks/inject-data-application/define-command-argument-container/&gt;overwrite&lt;/a&gt; the underlying Docker ENTRYPOINT and CMD arguments in your Docker image with the command and args settings in the . yaml file. Start the training by going by entering the following command: 1kubectl apply -f dialogue_train. yamlCheck that the pods statuses and ensure that they are starting or running by running kubectl get pods. If your code prints or displays the training progress, you can check on the progress via: 1kubectl logs my-deep-learning-training-kube-1&lt;/img&gt; You can also check whether your training has completed as well: 1kubectl get pods&lt;/img&gt;If Kubernetes is still running your training algorithm, the STATUS will display RUNNING instead. Details for Advanced Users: The earlier section was a basic introduction into how to deploy your deep learning algorithm into the Kubernetes cluster. However, there are intrincacies into how your training code should run while in the cluster. This is done in our example to highlight some of the best practises within the industry. Loading the Data from a Central Storage: Firstly, the Docker image size could potentially increase exponentially if the dockerfile includes the training dataset within the image built. To mitigate this, only codes (including cloning of code repositories via git clone) should be packaged into the Docker image. We recommend to to store your training data in the central S3 storage or the Hadoop cluster since these components were designed to store large amounts of data. Let us look at how to load our data from an S3 bucket. We use the boto3 python package to connect to the S3 bucket. 12345678910111213141516171819202122import boto3# Load the file from the S3 bucket. #s3_client = boto3. resource(  's3',  endpoint_url='http://minio. dsta. ai:9000' ,  aws_access_key_id='your_user_name',  aws_secret_access_key='your_password')s3_bucket = s3_client. Bucket('your_bucket_name')for obj in s3_bucket. objects. all():  key = obj. key    # Download the data. #  if key ==  data/movie_dialogue. pkl :    tmp_pkl = obj. get()[ Body ]. read()    data_tuple, idx2word, word2idx = pkl. loads(tmp_pkl)    # Download all codes. #  if key. find( codes/ ) != -1 and key. endswith( . py ):    local_file_name = key. replace( codes/ ,   )    s3_bucket. download_file(key, local_file_name)In our example, we also wrote our custom python script (tf_ver2_transformer. py) to implement a slightly modified version of the &lt;a href=https://arxiv. org/abs/1706. 03762&gt;Transformer&lt;/a&gt; (a final residual connection connecting the embedding inputs to the outputs is applied). Using s3_bucket. download_file(key, local_file_name), this script (and all other python scripts in the S3 bucket) is downloaded to the current working directory and directly imported into the main python program. The pickle file of the data movie_dialogue. pkl  is also read into memory using obj. get()[ Body ]. read(). When your training runs for a long time, it is desired to save your model parameters periodically. The trained model can then be downloaded directly from the MINIO interface and deployed to another machine if desired. This is useful, for example, when there is a need to deploy a trained model on-site without access to the AI platform. 1234567# Save the model to S3 bucket. #if n_iter % save_s3_step == 0:  model_files = [x[2] for x in os. walk(model_ckpt_dir)][0]  for model_file in model_files:    tmp_bucket_object = model_ckpt_dir +  /  + model_file    with open(tmp_bucket_object,  rb ) as tmp_model_file:      s3_bucket. Object(tmp_bucket_object). put(Body=tmp_model_file)Moving forward, we will be working on an approach to allow us to specify which training program to run as an argument to minimise building multiple Docker images which call the same building blocks. Once done, this article will be updated accordingly. That’s it! We hope that you enjoyed reading this article :). Appendix A:: The python is here. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259import osimport timeimport boto3import argparseimport numpy as npimport pandas as pdimport pickle as pklimport tensorflow as tffrom nltk import wordpunct_tokenize as word_tokenizer# Define the weight update step. ##@tf. functiondef train_step(  model, x_encode, x_decode, x_output,   optimizer, learning_rate=1. 0e-3, grad_clip=1. 0):  optimizer. lr. assign(learning_rate)    with tf. GradientTape() as grad_tape:    output_logits = model(x_encode, x_decode)        tmp_losses = tf. reduce_mean(tf. reduce_sum(      tf. nn. sparse_softmax_cross_entropy_with_logits(        labels=x_output, logits=output_logits), axis=1))    tmp_gradients = \    grad_tape. gradient(tmp_losses, model. trainable_variables)    clipped_gradients, _ = \    tf. clip_by_global_norm(tmp_gradients, grad_clip)  optimizer. apply_gradients(    zip(clipped_gradients, model. trainable_variables))  return tmp_losses# Parse the arguments. #parser = argparse. ArgumentParser()parser. add_argument(   -b ,  --batch , help= Batch size , required=True)parser. add_argument(   -d ,  --display , help= Display steps , required=True)parser. add_argument(   -n ,  --n_iterations , help= Number of iterations , required=True)args = parser. parse_args()print( Batch Size: , str(args. batch))print( Display steps: , str(args. display))print( No. of iterations: , str(args. n_iterations))# Load the file from the S3 bucket. #s3_client = boto3. resource(  's3',  endpoint_url='http://minio. dsta. ai:9000' ,  aws_access_key_id='your_user_name',  aws_secret_access_key='your_password')s3_bucket = s3_client. Bucket('your_S3_bucket')for obj in s3_bucket. objects. all():  key = obj. key    # Download the data. #  if key ==  data/movie_dialogue. pkl :    tmp_pkl = obj. get()[ Body ]. read()    data_tuple, idx2word, word2idx = pkl. loads(tmp_pkl)    # Download all codes. #  if key. find( codes/ ) != -1 and key. endswith( . py ):    local_file_name = key. replace( codes/ ,   )    s3_bucket. download_file(key, local_file_name)# The custom module can only be imported after downloading the  ## python scripts from the Minio server to the current directory. #import tf_ver2_transformer as tf_transformernum_data = len(data_tuple)vocab_size = len(word2idx)print( Vocabulary Size: , str(vocab_size))SOS_token = word2idx[ SOS ]EOS_token = word2idx[ EOS ]PAD_token = word2idx[ PAD ]UNK_token = word2idx[ UNK ]# Model Parameters. #batch_size = int(args. batch)seq_encode = 15seq_decode = 16num_layers = 6num_heads  = 16prob_keep  = 0. 9hidden_size = 1024ffwd_size  = 4*hidden_sizeinitial_lr  = 0. 001gradient_clip = 1. 00maximum_iter = int(args. n_iterations)restore_flag = Falsedisplay_step = int(args. display)save_s3_step = 10000cooling_step = 1000warmup_steps = 2500anneal_step  = 2000anneal_rate  = 0. 75model_ckpt_dir = \   TF_Models/transformer_seq2seq train_loss_file =  dialogue_train_loss_transformer. csv # Set the number of threads to use. #tf. config. threading. set_intra_op_parallelism_threads(4)tf. config. threading. set_inter_op_parallelism_threads(4)print( Building the Transformer Model.  )start_time = time. time()seq2seq_model = tf_transformer. TransformerNetwork(  num_layers, num_heads, hidden_size, ffwd_size,   vocab_size, vocab_size, seq_encode, seq_decode,   embed_size=hidden_size, p_keep=prob_keep)seq2seq_optimizer = tf. keras. optimizers. Adam()elapsed_time = (time. time() - start_time) / 60print( Transformer Model built (  + str(elapsed_time) +   mins).  )# Create the model checkpoint. #ckpt = tf. train. Checkpoint(  step=tf. Variable(0),   seq2seq_model=seq2seq_model,   seq2seq_optimizer=seq2seq_optimizer)manager = tf. train. CheckpointManager(  ckpt, model_ckpt_dir, max_to_keep=1)if restore_flag:  # Download all model outputs. #  for obj in s3_bucket. objects. all():    key = obj. key    if key. find(model_ckpt_dir) != -1:      s3_bucket. download_file(key, key)    ckpt. restore(manager. latest_checkpoint)  if manager. latest_checkpoint:    print( Model restored from {} . format(manager. latest_checkpoint))  else:    print( Error: No latest checkpoint found.  )    train_loss_df = pd. read_csv(train_loss_file)  train_loss_list = [tuple(    train_loss_df. iloc[x]. values) for x in range(len(train_loss_df))]else:  print( Training a new model.  )  train_loss_list = []# Placeholders to store the batch data. #tmp_input  = np. zeros([batch_size, seq_encode], dtype=np. int32)tmp_seq_out = np. zeros([batch_size, seq_decode+1], dtype=np. int32)tmp_test_in = np. zeros([1, seq_encode], dtype=np. int32)tmp_test_dec = SOS_token * np. ones([1, seq_decode], dtype=np. int32)n_iter = ckpt. step. numpy(). astype(np. int32)print( -  * 50)print( Training the Transformer Network ,     (  + str(n_iter),  iterations).  )  tot_loss = 0. 0start_tm = time. time()while n_iter &lt; maximum_iter:  step_val = float(max(n_iter+1, warmup_steps))**(-0. 5)  learn_rate_val = float(hidden_size)**(-0. 5) * step_val    batch_sample = np. random. choice(    num_data, size=batch_size, replace=False)    tmp_input[:, :]  = PAD_token  tmp_seq_out[:, :] = PAD_token  tmp_seq_out[:, 0] = SOS_token  for n_index in range(batch_size):    tmp_index = batch_sample[n_index]    tmp_i_tok = data_tuple[tmp_index][0]. split(   )    tmp_o_tok = data_tuple[tmp_index][1]. split(   )    tmp_i_idx = [word2idx. get(x, UNK_token) for x in tmp_i_tok]    tmp_o_idx = [word2idx. get(x, UNK_token) for x in tmp_o_tok]        n_input = len(tmp_i_idx)    n_output = len(tmp_o_idx)    n_decode = n_output + 1    tmp_input[n_index, :n_input] = tmp_i_idx    tmp_seq_out[n_index, 1:n_decode] = tmp_o_idx    tmp_seq_out[n_index, n_decode] = EOS_token  tmp_decode = tmp_seq_out[:, :-1]  tmp_output = tmp_seq_out[:, 1:]    tmp_loss = train_step(    seq2seq_model, tmp_input, tmp_decode, tmp_output,     seq2seq_optimizer, learning_rate=learn_rate_val)    n_iter += 1  tot_loss += tmp_loss. numpy()  ckpt. step. assign_add(1)  if n_iter % display_step == 0:    end_time = time. time()    avg_loss = tot_loss / display_step    tot_loss = 0. 0    elapsed_tm = (end_time - start_tm) / 60    start_tm  = time. time()    tmp_test_in[:, :] = PAD_token    sample_id = np. random. choice(num_data, size=1)    tmp_data = data_tuple[sample_id[0]]    tmp_i_tok = tmp_data[0]. split(   )    tmp_o_tok = tmp_data[1]. split(   )    tmp_i_idx = [word2idx. get(x, UNK_token) for x in tmp_i_tok]    n_input = len(tmp_i_idx)    tmp_test_in[0, :n_input] = tmp_i_idx        gen_ids = seq2seq_model. infer(tmp_test_in, tmp_test_dec)    gen_phrase = [idx2word[x] for x in gen_ids. numpy()[0]]    gen_phrase =    . join(gen_phrase)    print( Iteration , str(n_iter) +  : )    print( Elapsed Time: , str(elapsed_tm) +   mins.  )    print( Batch Size: , str(batch_size) +  .  )    print( Average Loss: , str(avg_loss))    print( Gradient Clip: , str(gradient_clip))    print( Learning Rate: , str(seq2seq_optimizer. lr. numpy()))    print(  )    print( Input Phrase: )    print(   . join([idx2word[x] for x in tmp_i_idx]))    print( Generated Phrase: )    print(gen_phrase)    print( Actual Response: )    print(tmp_data[1])    print(  )        # Save the training progress. #    train_loss_list. append((n_iter, avg_loss))    train_loss_df = pd. DataFrame(      train_loss_list, columns=[ n_iter ,  xent_loss ])    train_loss_df. to_csv(train_loss_file, index=False)    s3_bucket. Object( data/dialogue_train_loss. csv ). put(Body=train_loss_file)        # Save the model. #    save_path = manager. save()    print( Saved model to {} . format(save_path))        # Save the model to S3 bucket. #    if n_iter % save_s3_step == 0:      model_files = [x[2] for x in os. walk(model_ckpt_dir)][0]      for model_file in model_files:        tmp_bucket_object = model_ckpt_dir +  /  + model_file        with open(tmp_bucket_object,  rb ) as tmp_model_file:          s3_bucket. Object(tmp_bucket_object). put(Body=tmp_model_file)    print( -  * 50)"
    }, {
    "id": 19,
    "url": "https://dinohub.github.io/Training-A-ML-DL-Model-On-AIPlatform-Kubernetes-PartII/",
    "title": "Training a Open Source ML/DL model on AI Platform (Kubernetes) - Part II",
    "body": "2020/07/13 - Training a Open Source ML/DL model on AI Platform (Kubernetes) - Part II.                     Difficulty          Contact   Jax@Slack   Notice: Due to unforeseen circumstances, this example has yet to be tested on the AI Platform’s GPUs. The example you see here are based on CPU runs. Once the example is tested on the platform, this notice will be removed. Part I of this two part article series demonstrates a very simple example that runs a single iteration of a training model. In reality, this is very inefficient as most model training we have doesn’t take up an entire GPU resources that the AI Platform offers (V100, 32GB). This article will demonstrate how to better utilise a single V100 GPU when submiting a job to Kubernetes. This example is created based on the possibility to load multiple CUDA programs to run on the same GPU, albeit with questionable speed depending on your model’s complexity. (https://stackoverflow. com/questions/31643570/running-more-than-one-cuda-applications-on-one-gpu) Note that this article can be followed through without going through Part I. The additional remarks that are unique in Part II are highlighted in italic bold. Who should try this?: 3rd Parties: If you have a Deep Learning architecture you got from someone or pulled from open sourced research, and you need to perform some form of training on the model without intimate knowledge of the codes, this method would be most suitable for you. Development in Docker: For those who are regularly developing their codes in Docker, this would be very apt for them as well. The advantage of developing DL models in Docker is that they highly flexible when it comes to using different frameworks and versions. For example, you don’t have to crack your head on different versions of CUDA on your machine, just make sure you have a docker for every version. Most times, you don’t even have to worry about this as the frameworks such as Tensorflow come with their own docker images anyway. What will be achieved at the end of this article?: This example uses a 3rd party end to end image classification code. The code is customised to download dataset frmo S3 onto itself and also to upload model checkpoints and final results onto S3 after training. By the end of this article, you will get acquainted with very basic use of Docker and Kubernetes. You would be able to submit jobs to Kubernetes and get the results from S3 object stores. This article focused on how multiple training jobs can run at the samee time on the same GPU. While this example is demonstrated via hyperparameter runs, you may adjust your own configuration to perform other actions concurrently Overview: Preparation and then the model training: The above diagram is a visual representation of the steps depicted in this article. Do refer to it if you get lost along the way with the mountain of words. Prerequisites:  A client machine configured to connect to the Kubernetes cluster.  Docker installed on your own computer (Both Windows and Linux versions are fine)On your own computer:  Prepare your datasets Prepare your training codes Introducing bashful Prepare Dockerfile file Build a docker image Export/Save the docker image as a file_ Transfer to Kubernetes clientOn the Kubernetes client:  Load datasets onto S3 Load the docker image file as a docker image Push the docker image to the Docker Registry Prepare kubernetes job yaml file Run the job yaml fileStep-by-Step: On your own computer: Prepare your datasets: As we are submiting the jobs to the network for training, it means that your datasets needs to be accessible by several computers on the network via a shared storage. On the AI Platform, the following would be made available, S3, NFS and Hadoop. In this article, we will demonstrate with the use of S3 storage as it can cater to both structured and unstructured data. We would be loading datasets into S3 via a network connection, so its generally more efficient if you transfer files as a zipped archive rather than thousands of individual files unless your training codes do it differently. Do zip up your datasets if you can, however, if you have extremely large datasets, or structured data in databases, you can prepare the data in their native forms. Prepare your training codes: Your training codes should consist mainly of 3 parts.  Downloading of datasets from S3 Preprocessing and training of model Uploading of models and results data to S3Finally make sure your codes can run and train for at least an epoch to verify its working. A sample of a training code is found in image_classification_multi. py. The only change to this code to the original is such that the zipped datasets would be downloaded from S3 and then extracted for processing, after training, the model and results are saved in S3. Additionally, hyperparameters are received via argparse. Your situation could be different, please exercise your own considerations. A temporary MINIO S3 server has been setup in the AI Platform, your training codes should pull and save the data there. The s3utility. py script shows the related codes, this setups the helper codes and pulls the relevant parameters about the S3. The parameters are to be sent into the environment variables. You may also hard code the variables if that suits you, but i would encourage you to use either environment variables or argparse. 1234567891011121314151617181920212223242526272829303132### Setup of S3 parameters and helper functions#Names of the bucketstrainingbucket= os. environ['trainingbucket'] #'training'datasetsbucket= os. environ['datasetsbucket'] #'datasets'import boto3 #boto3 is S3 client from AWSfrom botocore. client import Configs3 = boto3. resource('s3',          endpoint_url= os. environ['endpoint_url'] ,          aws_access_key_id= os. environ['aws_access_key_id'] ,          aws_secret_access_key= os. environ['aws_secret_access_key'],          config=Config(signature_version= os. environ['signature_version']),          region_name= os. environ['region_name'])def s3_download_file(localfile,bucket,s3path):  print( S3 Download s3:// +bucket+ /  + s3path +   to   + localfile )  s3. Bucket(bucket). download_file(s3path,localfile)  def s3_upload_file(localfile,bucket,s3path):  print( S3 Uploading   + localfile +   to s3:// +bucket + s3path+localfile)  s3. Bucket(bucket). upload_file(localfile,s3path+localfile)  def s3_upload_folder(folder, bucket,s3path):    from glob import glob  print( Processing folder )  for file in glob(folder+ /**/* ,recursive=True):   if (os. path. isdir(file)) == False:     print( Processing   + file)    s3_upload_file(bucket='training',localfile=file,s3path='')This part of the code will save the results into S3. 1234567891011### Up until this point,allthe model files are saved on container. After this container finishes execution, the files will be gone. ### Start saving the checkpoints and model files. import jsonwith open('catdogclassification. json', 'w') as fp:  json. dump(history. history, fp)s3_upload_file(bucket=trainingbucket,localfile='catdogclassification. json',s3path='')s3_upload_folder(bucket=trainingbucket,folder='catdogclassification_model',s3path='')for epochrun in range(epochs):  s3_upload_file(bucket=trainingbucket,localfile='catdogclassification_save_at_'+str(epochrun+1)+'. h5',s3path='')A new python script download_datasets. py is created by extracting the data downloading codes Introducing bashful: bashful uses a yaml file to stitch together commands and bash snippets and run them, with the flexibility of doing it sequentially or concurrently. The bashful yaml for this example is located in bashful. yml. The config node should be reproduced in your own bashful. yml, but you are free to adjust the tasks node. Extract of this example’s bashful. yml 1234567tasks:  - name: download_datasets   cmd: python /download_datasets. py  - name: training   parallel-tasks:   - cmd: python /image_classification_multi. py --expid 1 --batch_size 128 --image_size_h 30 --image_size_w 30 --buffer_size 128 --dropout 0. 50 --epochs 1 --learning_rate 0. 01   - cmd: python /image_classification_multi. py --expid 3 --batch_size 64 --image_size_h 30 --image_size_w 30 --buffer_size 64 --dropout 0. 30 --epochs 1 --learning_rate 2. 80In the above yml, there are 2 main tasks, namely download_datasets and training. These are by default configured to execute sequentially. In the training task, there are 2 parallel sub tasks configured. Its essentially the script to run the training, which provided different hyperparamters. Say it is known that each model training will take up about 7GB of GPU RAM, you may run up to 4 parallel trainings (32mod7). Please note that the env variable TF_FORCE_GPU_ALLOW_GROWTH must be set to True if you are using Tensorflow so that not all the RAM is allocated to one process Prepare Dockerfile file: Now that you have your codes ready and tested locally, its time to dockerize it. Its really easy to create a Docker image, all you need is Docker installed, gather the files you want in the docker image and to create a simple file called Dockerfile. A Dockerfile is declarative, and the commands are only processed after you run docker build. **The following is the Dockerfile for this example. ** FROM tensorflow/tensorflow:nightly-gpuADD requirements. txt /RUN apt update &amp;&amp; \  apt install -y wget software-properties-common build-essential graphviz RUN wget https://github. com/wagoodman/bashful/releases/download/v0. 0. 10/bashful_0. 0. 10_linux_amd64. deb &amp;&amp; \  dpkg -i bashful_0. 0. 10_linux_amd64. debRUN pip3 install -r requirements. txtADD image_classification_multi. py /ADD s3utility. py /ADD download_datasets. py /ADD bashful. yml /Most Dockerfiles start off with a baseline image. There are a lot of images on DockerHub and chances are that there’s one that fits your purpose. Take for example, in this case the latest Tensorflow with GPU support is desired. Instead of creating a setup with CUDA and go through all the installation headache, a pre-made docker image by Tensorflow complete with CUDA and all is used instead. To do this, a FROM command followed by the tag tensorflow/tensorflow:nightly-gpu is used. Next, copy all the stuff required into the docker image by using the ADD command, followed by 2 arguments. The first argument is the path to the file, relative to the location of the Dockerfile file. The second argument is the path inside the docker image (The folders will be created automatically if it doesn’t exists). So it will look something like ADD requirements. txt /. **Note the differences with Part I. adding bashful executable and its yaml, adding s3utility. py and download_datasets. py ** The codes won’t run without the dependancies. In this example, graphviz and some python packages are quired. To this end, you can use the RUN command. For this example, use RUN pip3 install -r requirements. txt. After this is acheived, you may proceed to build the image. Build a docker image: To build the image with the docker file, you need to run the following command in the same folder where Dockerfile is located. In this example, under the kubejob/multi-train folder. 1234jax@getafix: docker build . t image-classification-multi ---&gt; ccd3aaa280b2Successfully built ccd3aaa280b2Successfully tagged image-classification-multi:latestYou can run docker images and see the docker image listed. 123jax@getafix: docker imagesREPOSITORY                    TAG               IMAGE ID      CREATED       SIZEimage-classification-multi.            latest              3793f6e38a2f    2 minutes ago    3. 49GBAt this point, you can run the docker image on your own computer and run the training. This is the closest to which how it will run on kubernetes. Successfullying running this step will ensure that your image will most likely run properly on kubernetes. 1jax@getafix docker run -it --gpus all --env-file env. list image-classification-multi /bin/bash -c  /bin/bash , -c , stty rows 25 &amp;&amp; stty cols 96 &amp;&amp; bashful run /bashful. yml --gpus all directs docker to use the GPU (provided nvidia-docker is installed) --env-file env. list loads the environment variables (S3 parameters) into the docker container.  /bin/bash , -c , stty rows 25 &amp;&amp; stty cols 96 is the command to start a new shell and default the tty dimensionsbashful run /bashful. yml is the command to run the bashful yaml When you run the command, you would see something like following (Note that this script ensures only 1 epoch is run for testing sake) 12345Running /bashful. yml  - download_datasets                                       - training           ⠴ ├─ python /image_classification_multi. py --expid 1 --batch_size 128 --image_size_h 30 -. . .   ⠴ ╰─ python /image_classification_multi. py --expid 3 --batch_size 64 --image_size_h 30 --. . . Export/Save the docker image as a file: The above steps ensured that you have a running training script that will work on Docker. The next step is to export the docker image so you can transfer it to the Kubernetes client. 1jax@getafix: docker save image-classification-multi -o image-classification-multi. tarNow transfer the docker image to Kubernetes client On the Kubernetes client: Upload datasets onto S3: On the Kubernetes client, a MINIO client (commandline) has been configured for you to manage your buckets. In this example, the following command would have been executed. 12345jax@getafix: /home/user/mc mb myminio/datasetsBucket created successfully `myminio/datasets`. jax@getafix: /home/user/mc cp kagglecatsanddogs_3367a. zip myminio/datasets/kagglecatsanddogs_3367a. zip. . . atsanddogs_3367a. zip: 786. 68 MiB / 786. 68 MiB ┃▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓┃ 55. 17 MiB/s 14sSome basic usage of the commmands are as follows. To create a new bucket ‘mynewbucket’, run the following command. 1/home/user/mc mb myminio/mynewbucketTo upload a folders or files ‘mylocalfolderorfile’, run the following command. 1/home/user/mc cp mylocalfolderorfile myminio/mynewbucket/To download a folder or file ‘mynewbucket’, run the following command. 1/home/user/mc cp myminio/mynewbucket/myremotefolderorfile mylocalfolderorfilePush the docker image to the Docker Registry: The docker image file that we copied over from our own computer needs to be loaded into the Docker Registry on the AI Platform. 12345678910111213141516171819202122232425262728293031#Loads the tar file (docker image) into the client's local docker repo. jax@getafix: docker load -i image-classification-multi. tar Loaded image: image-classification-multi:latest#Tag the uploaded image to bear the url to the AI Platorm's Docker Rgistry. jax@getafix: docker tag image-classification-multi dockrepo. dh. gov. sg/image-classification-multi:latest #Send the image from local Docker to the AI Platform's docker registry. jax@getafix: docker push dockrepo. dh. gov. sg/image-classification-multi:latest The push refers to repository [dockrepo. dh. gov. sg:5000/image-classification-multi94f43a58fd54: Layer already exists c394cd29e2f8: Layer already exists a7aabfd17751: Layer already exists be0113cc7bc0: Layer already exists 21f7133a99fb: Layer already exists 626976cc3d82: Layer already exists 63beefd08b72: Layer already exists e8f3214614e5: Layer already exists cc3fc5898d66: Layer already exists 7db070456ae6: Layer already exists 10a49ffdc6d4: Layer already exists 45a3946bc76a: Layer already exists 43895ac43b99: Layer already exists 808fd332a58a: Layer already exists b16af11cbf29: Layer already exists 37b9a4b22186: Layer already exists e0b3afb09dc3: Layer already exists 6c01b5a53aac: Layer already exists 2c6ac8e5063e: Layer already exists cc967c529ced: Layer already exists latest: digest: sha256:1df82e72ddb603195af7b57034d536190ccbc2c3ee59faed9a4844d3c079b8da size: 4515Prepare kubernetes job yaml file: The final step of preparation is to create a Kubernetes yaml file. 12345678910111213141516171819202122232425262728293031323334353637apiVersion: v1kind: Podmetadata:  name: multi-trainspec:     containers:     - name: test-image-classification-multi      image:  dockrepo. dh. gov. sg:5000/image-classification-multi       tty: true      env:      - name: SHELL       value:  /bin/bash       - name: CUDA_VISIBLE_DEVICES       value:  -1       - name: trainingbucket       value: training      - name: datasetsbucket       value: datasets      - name: endpoint_url       value: http://minio. dsta. ai:9001      - name: aws_access_key_id       value: user      - name: aws_secret_access_key       value: password      - name: signature_version       value: s3v4      - name: region_name       value: us-east-1      - name: TF_FORCE_GPU_ALLOW_GROWTH       value:  true       resources:       requests:         cpu:  1          memory:  1Gi       command: [ /bin/bash , -c , stty rows 25 &amp;&amp; stty cols 96 &amp;&amp; bashful run /bashful. yml ]     restartPolicy: NeverThe above yml is a minimal yaml required for this example, with the important ones stated below. image - Specify the image of that the job will run. env - List the environment variables required to pass to the container. requests - Minimum resources required for this container to run command - command to run the script. Due to a limitation in bashful, we need to start a tty and give it a size before running bashful, the command  /bin/bash , -c , stty rows 25 &amp;&amp; stty cols 96 &amp;&amp; bashful run /bashful. yml  does just that Run the job yaml file: Lastly, run the job submission. 12jax@getafix: kubectl apply -f kube-multi. ymlpod/multi-train createdWhen a kubernetes job has been successfully submited, you can monitor 2 things, as indicated below. Following command will display the pod that are in Kubernetes. The pod name takes after the meta-data &gt; name in the yaml file. 123jax@getafix: kubectl get podsNAME      READY  STATUS   RESTARTS  AGEmulti-train  1/1   Running   0     4sThe following command will display the running stdout of the pod that you just created. The output should be similar to the docker run output above. 123456jax@getafix: kubectl logs multi-trainRunning /bashful. yml  - download_datasets                                       - training           ⠴ ├─ python /image_classification_multi. py --expid 1 --batch_size 128 --image_size_h 30 -. . .   ⠴ ╰─ python /image_classification_multi. py --expid 3 --batch_size 64 --image_size_h 30 --. . . Looking forward: This is an example on running on a single GPU. Please watch out for more blog posts on distributed training. Call for contribution: The above example is one of many possible ways to utilise Kubernetes for our AI development. If you have an interesting idea, please feel free to share it on our Slack page. "
    }, {
    "id": 20,
    "url": "https://dinohub.github.io/powerful-things-markdown-editor/",
    "title": "Powerful things you can do with the Markdown editor",
    "body": "2018/06/12 - There are lots of powerful things you can do with the Markdown editor. If you’ve gotten pretty comfortable with writing in Markdown, then you may enjoy some more advanced tips about the types of things you can do with Markdown! As with the last post about the editor, you’ll want to be actually editing this post as you read it so that you can see all the Markdown code we’re using. Special formatting: As well as bold and italics, you can also use some other special formatting in Markdown when the need arises, for example:  strike through ==highlight== *escaped characters*Writing code blocks: There are two types of code elements which can be inserted in Markdown, the first is inline, and the other is block. Inline code is formatted by wrapping any word or words in back-ticks, like this. Larger snippets of code can be displayed across multiple lines using triple back ticks: 123. my-link {  text-decoration: underline;}HTML: 12345&lt;li class= ml-1 mr-1 &gt;  &lt;a target= _blank  href= # &gt;  &lt;i class= fab fa-twitter &gt;&lt;/i&gt;  &lt;/a&gt;&lt;/li&gt;CSS: 12345678. highlight . c {  color: #999988;  font-style: italic; }. highlight . err {  color: #a61717;  background-color: #e3d2d2; }JS: 123456789// alertbar later$(document). scroll(function () {  var y = $(this). scrollTop();  if (y &gt; 280) {    $('. alertbar'). fadeIn();  } else {    $('. alertbar'). fadeOut();  }});Python: 1print( Hello World )Ruby: 123require 'redcarpet'markdown = Redcarpet. new( Hello World! )puts markdown. to_htmlC: 1printf( Hello World ); Reference lists: The quick brown jumped over the lazy. Another way to insert links in markdown is using reference lists. You might want to use this style of linking to cite reference material in a Wikipedia-style. All of the links are listed at the end of the document, so you can maintain full separation between content and its source or reference. Full HTML: Perhaps the best part of Markdown is that you’re never limited to just Markdown. You can write HTML directly in the Markdown editor and it will just work as HTML usually does. No limits! Here’s a standard YouTube embed code as an example: "
    }, {
    "id": 21,
    "url": "https://dinohub.github.io/education/",
    "title": "powerful things you can do with the Markdown editor",
    "body": "2018/06/12 - There are lots of powerful things you can do with the Markdown editor If you’ve gotten pretty comfortable with writing in Markdown, then you may enjoy some more advanced tips about the types of things you can do with Markdown! As with the last post about the editor, you’ll want to be actually editing this post as you read it so that you can see all the Markdown code we’re using. Special formatting: As well as bold and italics, you can also use some other special formatting in Markdown when the need arises, for example:  strike through ==highlight== *escaped characters*Writing code blocks: There are two types of code elements which can be inserted in Markdown, the first is inline, and the other is block. Inline code is formatted by wrapping any word or words in back-ticks, like this. Larger snippets of code can be displayed across multiple lines using triple back ticks: 123. my-link {  text-decoration: underline;}If you want to get really fancy, you can even add syntax highlighting using Rouge.  Reference lists: The quick brown jumped over the lazy. Another way to insert links in markdown is using reference lists. You might want to use this style of linking to cite reference material in a Wikipedia-style. All of the links are listed at the end of the document, so you can maintain full separation between content and its source or reference. Full HTML: Perhaps the best part of Markdown is that you’re never limited to just Markdown. You can write HTML directly in the Markdown editor and it will just work as HTML usually does. No limits! Here’s a standard YouTube embed code as an example: "
    }, {
    "id": 22,
    "url": "https://dinohub.github.io/about-bundler/",
    "title": "About Bundler",
    "body": "2018/05/12 - gem install bundler installs the bundler gem through RubyGems. You only need to install it once - not every time you create a new Jekyll project. Here are some additional details: bundler is a gem that manages other Ruby gems. It makes sure your gems and gem versions are compatible, and that you have all necessary dependencies each gem requires. The Gemfile and Gemfile. lock files inform Bundler about the gem requirements in your site. If your site doesn’t have these Gemfiles, you can omit bundle exec and just run jekyll serve. When you run bundle exec jekyll serve, Bundler uses the gems and versions as specified in Gemfile. lock to ensure your Jekyll site builds with no compatibility or dependency conflicts. For more information about how to use Bundler in your Jekyll project, this tutorial should provide answers to the most common questions and explain how to get up and running quickly. "
    }, {
    "id": 23,
    "url": "https://dinohub.github.io/red-riding/",
    "title": "How to disply indent",
    "body": "2018/01/12 - An 1868 Prang catalogue would later claim that such “books in the shape of a regular paper Doll… originated with us”.  It would seem the claim could also extend to die cut books in general, as we can’t find anything sooner, but do let us know in the comments if you have further light to shed on this! Such books are, of course, still popular in children’s publishing today, though the die cutting is not now limited to mere outlines, as evidenced in a beautiful 2014 version of the same Little Red Riding Hood story.  12Simply by using &gt;. &lt;b&gt;&gt;&lt;/b&gt; It would seem the claim could also extend to die cut books in general, as we can’t find anything sooner, but do let us know in the comments if you have further light to shed on this! Such books are, of course, still popular in children’s publishing today, though the die cutting is not now limited to mere outlines, as evidenced in a beautiful 2014 version of the same Little Red Riding Hood story.  "
    }, {
    "id": 24,
    "url": "https://dinohub.github.io/options-for-creating-new-site-with-jekyll/",
    "title": "Options for creating a new site with Jekyll",
    "body": "2018/01/12 - jekyll new &lt;PATH&gt; installs a new Jekyll site at the path specified (relative to current directory). In this case, Jekyll will be installed in a directory called myblog. Here are some additional details:  To install the Jekyll site into the directory you’re currently in, run jekyll new . If the existing directory isn’t empty, you can pass the –force option with jekyll new . –force.  jekyll new automatically initiates bundle install to install the dependencies required. (If you don’t want Bundler to install the gems, use jekyll new myblog --skip-bundle. ) By default, the Jekyll site installed by jekyll new uses a gem-based theme called Minima. With gem-based themes, some of the directories and files are stored in the theme-gem, hidden from your immediate view.  We recommend setting up Jekyll with a gem-based theme but if you want to start with a blank slate, use jekyll new myblog --blank To learn about other parameters you can include with jekyll new, type jekyll new --help. "
    }, {
    "id": 25,
    "url": "https://dinohub.github.io/quick-start-guide/",
    "title": "Let's test spoilers",
    "body": "2018/01/11 - The mind-warping film opened with a hospital patient Simon Cable (Ryan Phillippe) awakening in a hospital with little knowledge (amnesia perhaps?) of what had happened, and why he was there, etc. He was told by attending Dr. Jeremy Newman (Stephen Rea) that it was July 29, 2002 (Simon thought it was the year 2000 - he was confused - he heard a doctor say 20:00 hours!) and that he had died for two minutes from cardiac arrest following the near-fatal accident – but he had been revived (“You’re as good as new”). Dr. Newman: “Simon, this is the 29th of July. The year is 2002. And your wife, whose name is Anna, is waiting outside. ” So how do we do spoilers?: 1&lt;span class= spoiler &gt;My hidden paragraph here. &lt;/span&gt;"
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}

function lunr_search(term) {
    $('#lunrsearchresults').show( 400 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-danger btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
    
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 5 );
        $( "body" ).removeClass( "modal-open" );
    });
});